{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 06:58:17.738576: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 06:58:17.739858: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 06:58:17.769926: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 06:58:17.771198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 06:58:18.246368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n",
      "[]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 06:58:19.812030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-17 06:58:19.972473: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-03-17 06:58:19.975594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-03-17 06:58:19.975616: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "if not hasattr(np, 'typeDict'):\n",
    "    np.typeDict = np.sctypeDict\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '/home/downtown/aiffel/BERT/kowiki.txt'\n",
    "model_dir = '/home/downtown/aiffel/BERT/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁1', '▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다', '었다', '▁지', '▁수', '▁19', '▁가', '▁시', '▁20', '▁기', '▁전', '▁아', '▁하', '▁있다', '▁다', '▁제', '했다', '하였', '▁일', '▁한', '▁중', '▁정', '▁주', '하는', '▁것', '▁자', '▁공', '▁인', '되었다', '▁경', '▁위', '▁유', '▁보', '하고', '▁3', '▁등', '▁부', '하였다', '▁조', '하여', '▁미', '▁동', '▁선', '▁나', '으며', '▁모', '▁연', '▁영', '▁의', '▁오', '▁마', '에는', '▁발', '▁소', '한다', '▁고', '▁개', '▁201', '▁구', '▁세', '▁도', '▁상', '▁비', '▁스', '▁국', '▁서', '▁후', '▁여', '▁200', '▁때', '▁4', '▁성', '▁해', '▁관', '▁있는', '▁신', '▁프', '▁대한', '부터', '▁5', '▁방', '▁또', '지만', '▁(', '▁역', '되어', '▁않', '▁만', '▁\"', '▁장', '▁바', '까지', '▁무', '▁남', '▁통', '▁현', '▁교', '▁같', '에게', '▁내', '학교', '▁문', '▁출', '▁거', '▁포', '▁결', '된다', '▁《', '적인', '이라', '▁6', '▁우', '적으로', '▁불', '▁원', '▁최', '▁10', '▁진', '▁생', '▁작', '▁어', '▁당', '국의', '▁노', '▁강', '▁알', '▁반', '▁7', '▁8', '▁계', '▁따', '▁파', '▁분', '▁없', '▁받', '▁말', '면서', '들이', 'or', '되었', '▁사용', \"▁'\", '▁두', '▁실', '에서는', '▁리', '들은', '▁명', '▁함', '▁단', '▁이후', '00', '하기', '▁예', '년에', '),', '▁한다', '▁안', '하게', '▁재', '▁9', '▁일본', '▁차', '▁설', '▁배', '▁에', 'er', '▁199', '▁다음', '았다', '▁특', '▁요', '였다', '▁18', '▁데', '▁종', '으나', '▁초', '▁군', '▁로', '▁독', '▁12', '()', 'la', '▁추', '▁김', '▁많', '▁그는', '▁~', '▁대한민', '▁되었다', '리아', '▁카', '▁활', '▁감', '▁라', '이며', '는데', '▁입', 'on', '▁건', '스트', '▁타', '▁다른', '▁형', '▁더', '▁지역', '▁S', '▁참', '▁운', '▁매', '▁같은', '▁태', '▁및', '▁그리', '▁양', '▁위해', '▁평', '▁음', '▁11', 'in', '▁행', '▁산', '주의', '▁적', '▁본', '▁)', '들어', '▁미국', '▁열', '▁과', '▁왕', '들의', '▁올', '▁승', '▁때문', '▁북', '▁크', '▁시작', '▁번', '기도', 'an', '▁프로', '▁코', '들을', '▁피', '▁호', '▁총', '레이', '▁C', '라는', '▁목', '되는', '▁화', '▁그러', '▁회', '▁베', '▁따라', '▁임', '▁각', '▁198', '▁또한', '하며', '▁변', '▁사람', '▁함께', '▁A', '▁표', '▁외', '▁속', '▁처', '▁f', '라고', '▁경우', '▁물', '▁달', '▁M', '이나', '▁위치', '▁현재', '▁약', '시아', '▁자신', '▁민', '▁직', '▁이름', 'mu', '하지', '▁새', '▁간', '▁학', '▁선수', '▁그의', '▁게', '▁F', '19', '▁드', '하다', '▁집', '▁기록', '▁영화', '▁당시', '되고', '▁한국', '▁for', '에도', '▁저', '▁항', '▁가장', '▁하나', '▁세계', '▁때문에', 'en', '▁사이', '기에', '▁그러나', 'mula', '▁존', '기를', '▁메', '▁점', '▁체', 'ar', '▁박', '▁뒤', '▁것을', '▁확', '▁15', '▁또는', '▁formula', '▁했다', '▁조선', '▁있으며', '스의', '▁것으로', '▁197', '▁된다', '▁국가', '▁있었다', '이라고', '▁의해', '▁T', '▁것이', '▁레', '▁16', '▁복', '▁청', '▁축', '▁토', '▁못', '▁투', '▁천', '▁광', '리는', '▁필', '지는', '▁그리고', '▁브', '일에', '▁통해', '▁활동', '졌다', '▁법', '▁합', '▁프랑', '▁철', '▁17', '▁야', 'al', '▁판', '등학교', '▁196', '▁근', '▁창', '▁증', '▁편', '▁삼', '했고', '▁디', '▁금', '▁B', '▁치', '▁살', 'ti', '▁많은', '▁것이다', '▁언', '리를', '▁들어', '▁첫', '이라는', '▁돌', '▁버', '▁심', '▁높', '▁대한민국', '▁아니', '▁다음과', '▁‘', '하였고', '▁쓰', '(,', '▁D', '랜드', '▁하는', '▁뜻', '하면서', '▁경기', '스는', '20', '▁충', '▁다시', '▁황', 'es', '▁플', '는다', '▁서울', '▁포함', '▁대해', '▁있었', '▁가지', '▁할', '이었다', '▁194', '▁I', '▁P', '▁여러', '▁애', '▁완', '▁네', '▁트', '▁K', '▁대한민국의', '대로', '▁14', '▁되', '사를', '▁병', '▁페', '로서', 'km', '▁백', '▁독일', '이고', '▁큰', '상을', '▁죽', '개의', '이는', '▁하였다', '되었고', '▁모두', '▁루', '▁패', '▁용', '▁연구', '보다', '▁순', '▁팀', '▁도시', '▁클', '▁권', '▁13', '▁사회', '▁195', '장을', '▁채', '위를', '▁은', '▁모든', '▁L', '▁히', '▁취', '▁처음', '▁알려', '성을', '니아', 'he', '지를', '▁규', '▁이루', '▁친', '▁시즌', '▁일부', '▁중국', '이었', '▁인구', '▁등이', '▁제작', '라이', '▁대표', '▁전쟁', '▁등의', '대의', '▁이용', 'ic', '회의', '▁감독', '▁정부', '▁식', '▁축구', '▁손', '리에', '자가', 'is', '▁아이', '▁위한', '▁존재', '년부터', '▁끝', '▁구성', '▁있어', '도의', '▁R', '부분', '했으며', '▁허', '▁정치', '▁즉', '▁이상', '도로', '▁영국', '▁G', '▁프랑스', '나라', '▁-', '력을', '▁이어', '▁테', '거나', '▁것은', '▁중심', '이자', '년에는', '▁일반', '▁문제', '▁자신의', '▁길', '▁같다', '▁맡', '▁나타', '시에', '▁월', '▁러', '▁가능', '▁온', '▁작품', '▁앞', '▁논', '▁뉴', '▁환', '▁N', '000', '▁게임', '기의', '▁개발', '▁걸', '▁맞', '▁리그', '▁곳', '▁이러', '▁동안', '▁갖', '▁193', '▁발견', '▁이러한', '▁기준', '▁캐', '▁접', '▁준', '리스', '▁H', '▁별', '▁새로', '▁하지만', '▁U', '▁주장', '▁30', '▁등을', '▁이는', 'at', '년대', '▁면', '▁위치한', '▁발표', '▁향', '▁E', '▁밀', '통령', '하면', '▁국제', 'le', '▁되어', '되었으며', '▁〈', '▁방송', '도록', '로부터', '▁농', '들과', '위원', '났다', '▁협', '(1', '▁지방', '시키', '▁볼', '로는', '▁잘', 're', '자의', '▁검', '▁의미', '기는', '▁2010', '▁섬', '▁이를', '▁대부분', '서는', '군의', 'it', '▁사건', '▁전투', 'ou', '리가', '기가', '다는', '▁공격', '에서도', '▁하고', '▁번째', '란드', '▁밝', '▁담', '월에', '▁폭', '사가', '▁육', '▁귀', '▁대회', '리카', '하였으며', '▁문화', '다고', '▁홍', '▁음악', '▁영향', '▁대학', '▁대통령', '▁막', '어진', '정을', '▁얻', '이트', '리그', '세기', '했던', '▁W', '년까지', '스를', '▁최초', '명의', '쪽으로', '▁참여', '▁미국의', '▁프로그', '▁석', '▁키', '▁극', '성이', '들에게', '▁설립', '▁날', '▁우승', '▁진행', '자를', '▁설치', '인의', '▁줄', '▁결과', '▁이에', '자는', '▁효', '▁특히', '▁앨', '와의', '도를', '▁발생', '10', '▁관계', '▁관련', '▁역사', '▁<', '▁일어', 'el', '▁곡', '▁배우', '▁된', '림픽', '▁2008', '▁책', '▁그녀', '디오', '▁절', '▁“', '▁봉', '▁25', '▁유럽', '▁사망', '▁발매', '리의', '▁만들어', '▁다양', '자로', '대를', '렸다', '가지', '▁암', '▁같이', '▁러시아', '▁이전', '▁생각', '이스', '▁독립', '▁형태', '▁부분', '(19', '▁갈', '므로', '요한', '▁태어', '▁시간', 'om', '▁사실', '▁폐', '▁글', '▁인해', '▁교육', '스터', '▁블', '▁헤', 'st', '▁주로', 'ol', '▁새로운', '▁소속', '▁질', '▁자유', '▁이것', '▁받아', '▁경제', '▁측', '▁2007', '▁좋', '▁수도', '▁지원', '▁2011', '▁받았다', '▁이탈', '▁있고', '스가', '▁2009', '르는', '▁기술', '▁올림픽', '라마', '▁2012', '▁중앙', '식을', '▁와', '▁계속', '과의', '▁침', '▁출신', '화국', '▁프로그램', '▁엔', '▁머', '▁t', '▁등장', '▁송', '번째', '▁지정', '▁잡', '▁로마', '▁커', '▁상태', '▁참가', '▁범', '▁구조', '▁있던', '도는', '▁누', '-1', 'ch', '▁192', '▁데뷔', '▁탄', '▁벌', '▁골', '학자', '▁2014', '▁결정', '▁매우', '▁성공', '다가', '사의', '▁선거', '▁탈', '부의', '▁운영', '▁난', '리즈', '인이', '이지', '대학교', 'am', '▁않았다', '프트', '▁아버', '▁혼', '으로서', '▁넘', '▁아들', '이션', '▁가지고', '르크', '▁J', '▁이탈리아', '▁2013', '전을', '▁21', ',000', '▁니', '▁타이', '드는', '▁몇', '비아', '▁일본의', '▁고려', '▁윤', '기로', '▁돌아', '▁없다', '명을', '▁급', '▁아니라', '▁슈', '하기도', '▁O', '▁들', '▁찾', '대에', '처럼', 'tion', '▁터', '에서의', 'ing', '▁차지', '해서', '▁결국', '▁만들', '장이', '▁100', '도가', '했지만', '▁24', '▁생산', '▁없는', '왔다', '▁한편', '되지', '▁운동', '▁악', '▁싱', '▁2015', 'us', 'et', '▁해당', '▁최고', '▁행정', '▁떨', '▁0', '▁조직', 'il', '▁2006', '▁출연', '남도', 'ro', '▁않고', '우스', '하자', '운데', '▁아래', '▁22', '▁23', 'FA', '▁케', '사는', '▁앨범', '▁전체', '▁계획', '권을', '▁업', '▁좌', '▁많이', '간의', '▁이유', '▁내용', '▁2016', '▁2018', '부에', '▁쿠', '원을', '냈다', '▁가리', '명이', '▁라이', '▁폴', '화를', '▁음반', '스타', '▁여성', '▁둘', '▁유명', '▁따르', '군은', '수를', '▁방법', '선의', '일부터', '수의', '▁힘', '\")', '▁가운데', '▁주요', '스크', '스템', '초등학교', '으로써', '원의', '▁떠', '▁하여', '▁후보', '▁빠', 'of', '▁팔', '▁대학교', '▁역할', '▁제공', '▁데이', '▁남아', 'as', '▁국민', '음을', '▁스타', '장으로', '어로', '장은', '▁2017', '▁다양한', '▁묘', '▁월드', '▁28', '진다', '▁몸', '▁26', '사로', '지의', '나는', '▁이루어', '▁않는다', '▁거의', '▁콘', '였던', '▁표현', '대학', '▁각각', '▁컴', '자인', '▁드라마', '하던', '▁인터', '▁잠', '리고', '민주', '▁27', '▁모습', '했으나', '▁헌', '이의', '인민', '인다', '지가', '▁스페', '운동', '▁수상', '(\"', '전에', '스와', '수가', '▁연결', '▁칼', '머니', 'ra', 'BS', '▁졸', '하려', '▁붙', 'de', '▁191', '▁마지', '됐다', '▁이름을', '▁이미', '으로는', '▁V', '▁격', '▁공식', '▁190', '_1', '▁경상', '▁,', '문화', '▁출전', '원이', '▁알려져', '▁얼', '▁임명', '▁필요', '▁2005', '▁시리즈', '스로', '성의', '▁따라서', '▁열린', '가는', '▁높은', '으로부터', '대표', '위원회', '점을', '▁작은', '▁마이', '▁몰', '력이', '▁마지막', '▁지구', '▁영향을', '▁시스템', '▁뛰', '제로', '▁인정', '▁공개', '▁직접', '▁과정', '▁정보', '▁능', '시켰', '지에', '▁2000', '대한', '▁빌', '▁따르면', '▁of', '었고', ').', '어를', '▁대신', '▁기원', '▁이야', '시의', '상의', '200', '▁보고', '이가', '▁2004', '▁개최', '지고', '▁진출', '▁추가', '▁대전', '▁기능', '북도', '▁항공', '▁센', '▁자연', '▁활약', '있다', '제를', '▁않은', '해야', '▁엘', '▁풍', '▁그리스', '▁평가', '》(', '군이', '▁c', '▁민주', '▁시대', '▁되는', '▁쇼', '▁29', '▁그룹', '▁정도', '▁있는데', '▁상황', '▁홈', '▁정의', '▁오스트', '▁녹', '▁생활', '▁유지', '▁브라', '▁눈', '▁바이', '▁그가', '갔다', '▁건물', '▁졸업', '▁말한다', '이드', '선을', '사에', '▁후에', 'ed', '▁변경', '▁비롯', '▁먹', '▁인도', '▁인구는', '▁철도', '▁서로', '▁노래', '▁역시', '수로', '▁발전', '▁크게', '▁퍼', '▁보여', '▁핵', '위에', '▁아르', '▁관한', '▁제국', '▁지금', '▁않았', '▁프리', '▁압', '▁중요한', 'ac', '르트', '▁여자', '▁레이', '▁응', '인을', '▁결혼', '전에서', '들에', '▁건설', '적은', '▁뿐', '▁인물', '▁거쳐', '▁울', '▁그것', '세의', '지로', '▁학교', '▁뜻은', '인은', '▁사업', '▁하였', 'ri', '▁부산', '부를', '일까지', '▁분류', '▁m', '원은', '▁낮', '계를', '▁[', '▁FC', '▁관리', '니다', '치는', '▁나라', '▁2019', '어는', '50', '식으로', '▁경기도', '▁상대', '▁혹', '▁이론', '특별', '▁일으', '▁공동', '킨다', '▁반대', '▁플레이', '▁요구', '▁가진', '▁형성', '▁희', '▁딸', '문을', '전히', '▁티', '광역', '임을', '▁밴', '▁만든', '▁원래', '▁연합', '원으로', '치를', '▁놓', '트로', '부는', '▁모델', '▁기업', '되면서', '▁31', '▁중화', '▁시절', '▁2002', '▁않는', '레스', '수는', '른다', '▁동시에', '▁색', '▁군사', '제가', '▁이르', '▁사이에', '▁느', '리오', '▁실시', '▁시작했다', '▁지하', '▁기본', '▁워', '운드', '군을', '하였으나', '▁겸', '장의', '인민공', 'ent', '▁싱글', '▁2003', '대는', '▁어떤', '상이', '▁지역의', '▁비판', '▁승강', '▁아닌', '이었던', '▁제외', '▁노동', '▁:', '역이다', '대가', '프리카', '회를', '▁계약', '▁긴', '▁잉', '제의', '▁역할을', '▁운행', '정한', '▁현대', '▁학생', '하는데', '상으로', '▁자동', '▁50', '▁싸', '▁민족', '▁명칭', '▁지도', '▁소련', '▁야구', '이기도', '▁189', '▁전통', '▁처음으로', '▁여기', '주는', '▁최초의', 'and', '장에', '▁전라', '체의', '명은', '▁콜', '▁판매', '나다', '▁보통', '▁공간', '▁휴', '▁영화이다', '스탄', '▁이동', '▁이때', '▁망', '되며', '린다', '있는', '▁특징', '▁수행', '▁의한', '▁있을', '▁나오', '12', '▁「', '▁기준으로', '이를', '18', '▁기반', '▁기간', '▁넓', '▁도입', '▁달리', '▁되고', '단의', '▁그래', '▁초기', '▁특별', '자와', '▁견', '▁교황', '호로', '▁비슷', '▁전문', '▁옮', '▁도쿄', '소를', 'un', '키는', '▁서비', '▁푸', '▁개인', '카이', '▁있다고', '▁인간', '과는', '▁하며', '▁마을', '▁위하여', '적을', '▁기존', '▁종교', 'ig', '연구', 'BC', '.5', '격을', '서를', '성은', '▁다이', '▁퇴', '관은', '▁웨', '시마', '▁바로', '장에서', '▁개의', '▁혹은', ')·', '▁엄', 'ur', '▁지나', '▁혁', '호를', '체를', '15', '▁역임', '독교', '▁궁', '▁월드컵', '중에', '▁애니', '▁슬', '▁벨', '▁탑', '가와', '물을', '▁과학', '▁모양', '▁훈', '▁빈', '시간', '▁s', '▁출시', 'os', '▁1990', '▁목록', '▁기타', 'um', '▁정책', '▁어머니', '.\"', '▁사랑', '▁유일', '▁태양', '고등학교', '혔다', '동안', '▁지배', '무를', '트를', '명이다', '퓨터', '▁이끌', '▁아버지', '▁국내', '30', '▁일반적으로', '드로', '했는데', '드의', '▁전국', '▁될', '텔레', '▁경기에서', '하지만', '▁연방', '▁경찰', '▁기관', '광역시', '▁과거', 'od', '리로', '▁언어', '▁소설', '▁지휘', '▁주변', '16', '▁공연', '▁맥', '리와', 'ation', '도에', '화가', '▁동일', '▁기원전', '기관', 'ot', '▁샤', '▁p', '▁중심으로', '트의', '▁부르', '마다', '피언', '학과', '방송', '▁이적', '쪽에', '시켰다', '▁40', '▁산업', 'ad', '▁2001', '드를', '▁이것은', '▁보내', '▁그를', '▁받은', '스코', '▁적용', '▁탐', '▁가수', '▁비교', '▁FI', '의를', '▁흔', '구의', '▁먼', '군에', '▁1,', '▁곧', '호는', '▁획', '▁뮤', '메이션', '▁풀', '위로', '▁속하는', '경기', '▁받았', '▁추정', '▁밖', '▁작업', '▁지정되었다', '▁있다는', '▁확인', '▁분야', '▁클럽', '▁충청', '▁가까', '쳤다', '▁담당', '▁떨어', 'id', 'ter', '▁선정', '▁받고', '▁방영', '▁the', '▁찾아', '▁건축', '류의', '▁컴퓨터', '▁없이', '동에', '▁되었고', '체가', '리지', 've', '▁노선', '▁숙', '▁착', '▁국가대표', '▁옥', '당의', '▁방향', '▁챔', '▁때문이다', '▁기독교', '년의', '▁규모', '▁열차', '▁작곡', '▁피해', '어가', '▁자기', '트는', '▁통합', '▁득', '사와', '▁환경', '-2', '▁지지', '비전', '▁낙', '▁염', '▁차량', '▁증가', '▁배우이다', '14', '정에', '단을', '세는', '야마', '▁어느', '▁두고', '▁실패', '스에', 'ow', '▁설명', '하거나', 'op', '▁가족', '▁대부분의', '▁그녀는', '글랜드', '어의', '60', '조의', '크로', '▁우주', '▁멤', '인으로', '▁개념', '▁넣', '르게', '▁너', '▁기념', '▁늘', '▁올라', '▁활동을', '종의', '▁태어났다', '▁갑', '▁뉴욕', '▁이런', '11', '▁윌', '▁상당', '▁챔피언', '▁오늘', '부가', '▁최대', '▁자신이', '▁이와', '중학교', '나무', '▁수록', '▁이상의', '▁보호', '제는', 'ec', '.2', '회는', '▁캐나다', '인이다', '동을', '▁있지만', '시켜', '▁애니메이션', '▁걸쳐', '학적', '이로', '▁이름은', '▁서울특별', '▁TV', '트남', '▁1980', '▁FIFA', '일에는', '▁데이터', '▁사상', '면에', '▁사고', '▁(19', '▁하나이다', '▁어린', '원에', '▁대체', '교육', '▁다음을', '계의', '▁없었다', '▁1999', '관을', '▁있어서', '렇게', '움을', '▁수는', '▁등에', '으면', '▁대하여', '적이', 'cm', '▁멤버', '▁·', 'em', '세가', '▁스페인', '겼다', '▁1998', '식의', '▁최초로', '▁예정', '▁최근', '스카', '이란', '▁등으로', '구에', '▁있었던', '치가', '▁하다', '13', '▁뛰어', '메리카', '▁흐', '▁나가', '▁획득', '▁위원', '했다고', '성과', '일을', '17', '기와', '▁땅', 'ag', '▁선출', '▁자료', '80', '심을', '자들은', '.)', 'ir', '▁더욱', '▁불가', '▁공화국', '드리', '▁잉글랜드', '식은', '▁윈', '▁텔레', '▁개봉', '▁선택', '▁최종', '▁내려', '단은', '▁구간', '_2', '식이', '성에', '했을', '▁지역에', '세를', '지역', '▁베이', '▁인천', 'oc', '▁맺', '어서', '▁부족', '▁흑', '▁솔', '▁잔', 'ia', '▁조사', '▁조건', '인민공화국', '▁보인다', '▁시민', '널리', '▁평균', '▁d', '▁정당', '▁선언', '학을', '트리', '40', '▁물리', '▁변화', '▁앤', '▁b', '▁런', '▁조지', '시키는', '되었으나', '.0', '▁연주', '▁자리', '▁명령', 'th', '▁대구', '▁선발', '프로', '▁어려', '▁흥', '▁1997', '한다는', '▁공산', '덜란드', 'ver', '개월', '》,', '시대', '▁동물', '▁연속', '▁에너', '▁도로', '▁서울특별시', '난다', '렀다', '▁결승', '▁가리킨다', '▁점령', '▁칭', '정부', '▁있게', '함으로써', '▁이름으로', '소가', '▁불구', '▁효과', '라도', '만을', '▁작가', '▁1995', '▁반응', '▁전기', '호선', '▁표준', '명으로', '▁Y', '▁움', '▁이들은', '▁옛', '▁관할', '포츠', '▁잃', '년간', '▁거주', '▁좋은', '▁국립', '▁X', '자들이', '하도록', '▁예를', '▁헌법', '내는', '만이', '▁언급', '▁몽', '▁교통', '상에', '이지만', 'ce', '▁이유로', '▁1996', '회에서', '▁팬', '▁이해', '▁아시아', '▁선수이다', '▁컨', '구를', '▁현재는', '이에', '▁고대', '▁유사', '▁후반', '▁덕', '▁마르', '▁덴', '쪽으로는', '▁국가대표팀', '▁그들의', '▁승리', '상은', '▁전에', ']]', '▁징', '▁네덜란드', '웠다', '▁익', '비를', '▁액', '▁소유', '▁주의', '역을', '▁출판', '▁베트남', '▁왜', '인데', '▁전자', '▁조약', '▁단체', '▁목표', '▁메이', '▁정규', '▁폴란드', '스턴', '▁사무', '레일', '▁겨', '▁km', '▁북한', '었으며', '진을', '▁것이라고', 'nd', 'mm', '▁휘', '학의', '모토', '▁면적은', '▁예술', '으므로', '▁공립', '일랜드', '▁명의', '▁세포', '▁지속', '▁이곳', '▁입단', '▁하나의', '▁맡았다', '▁주민', '주에', '▁188', '과에', '▁현재의', '▁1994', '랐다', '▁시설', '▁구분', '라의', '▁모습을', '월에는', '▁쌍', '▁깊', '▁밤', '▁이듬', '▁밝혔다', '▁국회의', '▁하지', '▁버전', '▁정신', '▁영국의', '정이', '▁하나로', '▁1970', '민주주의', '▁정확', '▁드러', '▁우리', '▁불구하고', '▁돈', '▁사용하는', '▁대중', '▁아직', '베이', '되기', '▁지상', '▁말했다', '▁끌', '▁실험', '▁소프트', '▁실제', '▁오늘날', '▁소비', 'ay', '▁강화', '지지', '▁그런', '▁1992', '▁분리', '▁쓴', '70', '계에', '▁씨', '라스', '지방', '자리', '..', '▁포르', '▁제한', '▁유래', '사카', '▁[[', '▁그림', '원에서', '▁세워', '▁거두', '▁이듬해', '▁육군', '▁사이의', '나이', '▁믿', '단에', '▁크리스', '스키', '관이', '치고', '주시', '▁핀', '직을', '지와', '▁페르', '금을', '▁가문', '▁웹', '메달', '소의', '버스', '▁알려진', '단이', '▁은퇴', '▁불리', '▁언론', '시코', '▁고등학교', '회에', '▁통과', '부르크', '거리', '월부터', '스테', '▁비해', '▁찬', 'The', '▁분석', '가의', '골을', '▁위에', '▁널리', 'im', '▁남자', '▁포함한', '회가', '형을', '▁그들은', '▁의미한다', '들로', '▁제품', '▁물질', '톨릭', '▁방문', '베르', '▁나온', '▁발달', '▁헬', '▁칸', '▁펼', '▁움직', '▁회사', '▁등과', '장과', '정의', '▁중에', 'ul', '바이', \")'\", '▁경험', '로의', '▁1991', '철도', '▁차례', '▁이사', '▁결합', '스티', '▁못했다', '▁완전히', '▁제조', '체는', '▁홀', '법을', '▁비행', '▁설계', '▁크로', '▁바탕', '불어', '하의', '▁자치', '▁친구', '▁프로젝', '르고', '▁것에', '드가', '루스', '이어', '▁여겨', '▁1993', '-0', '▁전해', '▁꽃', '▁괴', '▁사람이', '▁연기', '▁요청', '▁갖고', '▁부여', 'te', '▁배치', '▁더불어', '들도', '아의', '▁통일', '▁멸', '▁자주', '▁때까지', '▁소재', '당은', '▁1960', '▁마음', '▁스웨', '러스', '선에', '▁정도로', '점이', '▁알렉', '▁독일의', '사업', '▁수준', '▁프랑스의', '▁비롯한', '▁주연', '일본', '▁빛', '시는', '▁런던', 'EFA', '▁브라질', '산을', '▁맨', '▁멀', '▁깨', '소는', '▁제안', '▁나머', '▁오른', '▁중요', '▁그대로', '찬가지', 'TV', '▁영토', '▁목적으로', '▁마찬가지', '▁1988', '▁초등학교', '▁오래', '▁파괴', '▁요시', '로를', '▁쉽', '▁바뀌', '릭터', '▁유전', '트가', '▁아프리카', '▁감독의', '▁스스로', '▁이로', '위가', '부로', '▁주인', '경을', '▁텔레비전', '스토', '▁않아', '웨어', '▁인터넷', '▁만나', '▁유명한', '▁60', '위는', '▁확장', '지에서', '기술', '우는', '▁있도록', '▁외국', '▁패배', '관으로', '》()', '▁성장', '.3', '▁이들', '▁식민', '인과', '정은', '▁추진', '▁밝혀', '선거', '가를', '▁오스트리아', '었던', '▁의하여', '▁많다', '▁치료', 'ap', '\",', '▁스웨덴', '문에', '위의', '▁존재한다', '▁사라', '선이', '▁달러', '▁UEFA', '▁하이', '물의', '▁지난', '▁우승을', '▁아마', '주를', '▁남부', '▁기여', '▁사람들이', '▁칠', '▁가르', '받았다', '▁복귀', '되기도', '일의', '지게', '감을', '르의', '▁불교', '아가', '▁경우가', '▁경쟁', '▁1989', '종이다', '▁혈', '없이', '▁스포츠', '영화', '▁과정에서', '신을', '▁강력', '▁근처', '▁법률', '▁습', '▁몬', '▁짧', '▁되었', '▁먼저', '▁일어나', '▁하계', '▁캐릭터', '▁공항', '▁해석', '▁왕국', '회사', '▁있으나', '▁이외', '▁초대', '라인', '멕시코', 'NA', '▁주도', '▁것이었다', '많은', '▁영어', '▁접근', '48', '▁수용', '▁가톨릭', '▁광주', '규모', '▁폐지', '력은', '▁집합', '▁일제', '▁밴드', '▁서식', '▁들어가', '▁관측', '▁불리는', '▁품', '▁선수로', '이기', '카와', '하라', '스에서', '레일리아', '▁KBS', '▁가능하다', '▁단순', '군과', '도와', '▁알고', '조는', '▁그녀의', '▁잇', '▁작용', '▁제작된', '▁개혁', '▁특정', '▁촉', 'li', '구는', '행을', '▁스위', '정보', '▁않다', '25', '오는', '▁가져', '디어', '▁구성되어', '▁기초', '형의', '▁필리', '화국의', '▁이야기', '▁도착', '▁십', '▁인민', '▁노르', '▁때는', '▁결성', '하다가', '▁여기서', '과학', '▁교수', '하이', '▁제국의', '소년', '▁비슷한', '자이다', 'og', '▁70', '주로', '▁여객', '▁성립', '▁술', '▁St', '졌고', '▁/', '▁강원', '▁자동차', '▁조각', '량이', '간다', '▁186', '▁해결', '▁궤', '▁조선민주주의', '▁선수권', '▁수많은', '수한', '전의', '▁거부', '▁록', '▁떨어진', '▁스테', '하였는데', '▁아카', '크의', '▁해안', '▁경영', '_3', '▁앙', '웨이', '▁사용한다', '▁디자인', '▁파견', '▁기록했다', '▁상대로', '▁삼성', '▁반란', '속도로', '고를', '▁아무', '시를', '▁지역에서', '▁라디오', '▁바꾸', '▁지냈다', '했다는', '▁행동', '선은', '▁상호', '▁생성', '▁의하면', '▁보다', 'ab', '▁동맹', '▁『', '▁다르', '▁발행', '▁겪', '▁중간', '▁아시', '▁히로', '▁여름', '▁미국에서', '▁북쪽', '▁그린', '▁이루어진', '▁살아', '▁놀', '전이', '▁외교', '어나', '▁187', '▁의원', '▁대표적인', '▁을', '▁남쪽', '▁황제', '▁프레', '90', '▁제시', '▁부정', '▁당선', '센터', '구가', '하기로', '▁나무', '▁대해서', '▁승강장', '▁훈련', '▁슈퍼', '▁지하철', '함을', '▁공작', '▁임시', '▁분포', '자에', '▁타이틀', '▁다만', '▁시행', '▁만에', '▁2020', '촬영', '▁논란', '량을', '▁것과', '▁마찬가지로', '▁오스트레일리아', '▁1945', '(200', '▁형태로', '▁이렇게', '▁투표', '▁근거', '▁위해서', '▁엔진', '미터', '▁in', '부에서', '▁방어', '▁이주', '▁상징', '▁처리', '트워', '관의', 'ard', '▁흡', '▁캠', '▁중에서', '▁빅', '와는', '인트', '▁번역', '▁위험', '아시아', '▁계열', '▁대응', '▁시작하였다', '낸다', '▁교회', '▁일을', '▁1950', '▁끝에', '(20', '▁윌리', '만의', '▁스트', '전은', '왕의', '▁1986', '나가', '▁출생', '▁등록', '▁유니', '았고', '▁부른다', '▁멕시코', '▁1987', '▁디지', '▁년', '지션', '▁표시', '▁벽', '▁개통', '▁야마', '였고', '한다고', '▁시도', '▁n', '▁재판', '업을', '▁제주', '교회', '면을', '시즌', '으면서', '▁붕', '▁꾸', '▁체포', '성으로', '▁준비', '▁사용된다', '▁값', '평양', '▁곳에', '▁즉위', '▁몇몇', '▁있었고', '▁뇌', '▁통치', '조선', '▁있기', '크라', '▁규정', '▁?', '▁삶', '화된', '.1', '.4', '사에서', '▁한국의', '▁아일랜드', '사회', '▁이름이', '▁숨', '21', '▁적이', '▁사용할', '▁곤', '점으로', '▁좋아', '▁시험', '조를', '년을', '요일', '▁회의', '어졌다', '23', '▁활용', '▁쌓', '▁체결', '▁동생', '▁바다', '▁보유', '리에서', '문화재', '▁중단', '24', '▁따른', '▁캘', '디아', '▁제기', '▁대해서는', '▁벤', '화의', '오카', '.6', '86', 'ut', '▁그렇', '학생', 'ie', '▁있지', '▁낳', '대와', '▁본래', '▁1984', '▁보존', '▁다카', '▁미사', '▁안정', '▁다루', '▁종합', '▁나타나', '인들이', '▁조성', '▁가정', '▁직후', '▁귀족', '▁80', '▁유로', '▁경상북도', '▁윈도', '나의', '연합', '▁카를', '▁관련된', '▁종류', '▁밑', '물이', '▁나머지', '부와', '국이', '당이', '해졌다', '▁얼마', '▁시기', '▁지적', '량은', '▁표기', '▁경상남도', '▁계승', '수이다', 'ine', '가이다', '도시', '▁살해', '▁주인공', '▁촬영', 'ht', '▁것도', '▁오브', '▁못하고', '35', '▁완성', '.7', '▁보수', '▁센터', '가로', '습니다', '▁하루', '▁1985', '▁팀의', '▁문학', '▁엘리', '▁서비스', '▁and', '▁못한', '▁떠나', '▁혁명', '였으며', '▁옮겨', '▁사진', '▁내각', '리포', '로가', '니는', '▁동쪽', '▁오후', 'ion', '▁수비', '력의', '나라의', '트에서', '러운', '▁대사', '▁둔', '▁강한', '되자', 'ess', '하에', '▁천황', '▁가능한', '▁금지', '▁대륙', '▁혐', '역으로', '▁유일한', '▁이루어져', '▁위치하고', '▁싶', '인지', '시키고', '전한', '권의', '선에서', '롭게', '▁홍콩', '자들의', '소에', '내고', '▁묘사', '시킨', '▁이스', '▁취임', '안을', '소로', '주교', '▁윌리엄', '▁안에', '▁류', '▁달성', '▁왼', '체로', '려고', '▁유대', 'nt', '서로', '▁비디오', '▁젊', '▁디지털', '▁바르', '국은', '▁해외', '▁스코', '▁의견', '계가', '▁종료', '▁포르투', '티아', '學校', '▁충돌', '▁정부는', '▁르', '▁베르', '▁물론', '▁이용하여', '력으로', '▁벗', '▁스튜', '▁친일', '89', '▁각종', '▁일어난', '▁소개', '▁인근', '▁중의', '▁경우에는', '▁이집', '▁지명', 'br', 'ak', '민족', '치의', '피아', '▁전달', '▁근무', '▁사이에서', '▁필요한', '▁간의', '▁아내', '▁35', '▁위치해', '▁승격', '▁문제가', '▁오사카', '세에', '▁주목', '▁관광', '헝가', '▁억', '▁2,', '▁경기를', '▁가까운', '당을', '▁발사', '▁지역을', '월까지', '로로', '▁빨', '치로', '▁낮은', '▁읽', '▁참여하였다', '▁말을', '▁도시로', '▁32', '▁제거', '임스', '▁베네', 'ity', ')\"', '▁죄', '▁아니다', '▁나누', '▁파이', '▁관계를', '▁위의', '▁전환', '▁듣', '▁이슬', '분을', '적으로는', '조가', '92', '▁1982', '▁합류', '45', '분에', '▁만들었다', '▁남북', '▁둘러', '▁가입', '▁뒤에', '▁알렉산', '▁문제를', '량의', '▁만들어진', '▁폰', '▁썼', '단으로', '▁굴', '작용', '▁중국의', '▁미술', '▁해군', '▁서쪽', '기업', '▁빼', '▁이래', '승을', '워드', '▁명예', '고로', '▁만화', '라우', '▁에이', '▁되었으며', '▁평화', '▁후쿠', 'ph', '▁제도', '▁수가', '▁꼬', '▁출신의', '▁주는', '준다', '니스', '▁불린다', '▁건너', '▁구조를', '▁즐', '▁Ch', \"',\", '▁북부', '.(', '▁갖추', '물관', '그룹', '▁뮤직', '라고도', '▁1983', '▁바탕으로', '▁네트워', '▁미래', '▁정식', '지도', '▁그려', '▁반면', '▁계산', '▁실제로', '석을', '▁옆', '론을', '▁실행', '▁멜', '▁신라', '▁전반', '상과', '▁전시', '▁전략', '되었던', '▁펜', '▁종목', '종이', '▁최고의', '▁너무', '▁마련', '▁정치적', '▁기존의', '▁제목', '인들은', '▁만약', '만에', '일은', '문학', '▁전라남도', '메이', '▁w', '▁부문', '계는', '▁주었다', '신의', '▁오르', '▁In', '권이', '85', '▁사실을', '▁납', '프는', '▁재직', '▁MBC', '▁그해', '▁비밀', 'ist', '시가', '▁기록을', '▁저항', '산으로', '버트', '▁스토', '▁집중', '▁신문', '▁강조', '이던', 'ge', '▁The', '▁통하여', '▁사람은', '▁가는', '▁인식', '산의', '▁부모', '▁넘어', '리어', '미널', '▁맹', '▁배경', '▁생물', '▁연장', 'ber', '▁기후', '산업', '\"(', '즈의', '사이', '먼트', '상에서', '▁작성', '-3', 'ne', '생활', '관리', '▁아키', '메달을', '▁상업', '▁던', '▁사령', '이라고도', '▁정착', '▁마이크로', '점에서', '▁불러', '▁성격', '비가', '크는', '했기', '▁무렵', '▁II', '▁인한', '로스', '년대에', 'CA', '차를', '▁적극', '▁기능을', '▁끼', '▁제외한', '▁하기', '라노', '▁보여주', '▁영역', '질을', '세포', 'est', '▁허용', '▁혜', '▁객', '단체', '▁공급', '보다는', '▁아우', '졌으며', '해를', '▁올랐다', '은행', '교의', '공항', '두고', '시대의', '장이다', '로운', '▁로스', '▁내부', '▁확대', '▁셀', '▁뿐만', '에게는', '▁유형', '▁왔다', '법원', '▁대회에서', '파의', '▁이후에', '전쟁', '47', '▁최소', '▁입학', '▁사용하여', '▁사회주의', 'all', '▁일반적인', '▁나중에', '▁일이', 'ran', '▁의사', '▁오스', '▁사람을', '밖에', ')》', '계에서', '프가', '▁항상', '▁헨', '▁포지션', '22', '▁받아들', '▁정부의', '▁생활을', '▁이슬람', '▁1979', '리포니아', '▁여전히', '▁지진', '▁뒷', '▁족', '▁터키', '▁만드는', '.8', '▁부근', '적이고', '가가', '물로', '▁1981', '▁국방', '▁보면', '▁인디', '▁관찰', '▁전투에서', '르타', '▁이번', '▁제정', '▁케이', '▁그래서', '래프트', '되었는데', 'qu', '.9', '▁시대의', '▁판단', '세이', '▁출발', '▁보도', '▁동료', '▁동부', '▁연극', '▁1972', '년과', '▁원자', '▁확보', '▁핀란드', '호가', '과를', '▁위협', '▁있었지만', '개발', '기고', '▁편집', '▁채널', '▁매년', '원과', '주가', '▁사용되는', '한국', 'ers', '▁a', '▁덴마', '▁보이', '▁붕괴', '▁선의', '▁가리키는', 'ub', '▁바람', '▁쉽게', '▁포르투갈', '▁국왕', '▁뒤를', '면의', '▁등은', '시오', '다이', '▁경우도', '27', '▁정리', '사령', '▁de', '까지의', '▁채택', '로도', '서관', '▁마리아', '았으며', '▁문서', '26', '끄는', '▁젊은', '력에', '▁충분', '▁90', '▁동계', '▁교체', '▁전개', '36', '▁마사', '동차', '▁흰', '▁높이', '실을', '▁이승', '경제', '▁방식', '▁시의', '시로', '▁범죄', '▁시작되었다', '▁1973', '▁합병', '▁500', '▁세운', '▁유리', '▁남동', '대전', 'tic', '▁승리를', '▁끊', '▁모스크', '▁일종이다', 'ST', '▁남편', '▁외에도', '▁말이다', '▁공격을', '▁겨울', '▁다섯', '▁감소', 'art', '▁하자', '▁점차', '37', '▁아나', '▁장기', 'ell', '▁척', '세기에', '▁시기에', '46', '구려', '▁없어', '▁득점', '모리', '화에', '▁떨어져', 'FC', '라운드', '▁계약을', '▁자리를', '▁밖에', '력과', '비는', '▁탑재', '▁측정', '트라', '▁귀국', '당에', '드와', '▁강력한', '로서의', '87', '▁옹', '배우', '▁등에서', '▁전사', '▁의해서', '▁중학교', '▁사람들은', '▁울산', '리기', '▁사용한', '▁서부', '▁블랙', '▁k', '음악', '▁구축', '▁그곳', '의로', '▁녹음', '▁잠시', '하려는', '▁강원도', '▁민간', '▁수학', '대에서', '테인', '▁본관은', '▁얼굴', '▁수립', '▁관심을', '▁냉', '데미', '▁갖춘', '▁하게', '▁성우', '▁책임', '까지는', '▁세르', '▁설립된', '▁에드', '자치', '▁하면서', '계로', '▁당시의', '▁헝가', '문이', '▁캘리포니아', '▁안전', '▁정보를', '▁통신', '▁충청남도', '▁해체', '▁필리핀', '당한', '▁구역', '▁주장했다', '▁변호', '협회', '▁시청', '▁흘', '▁인수', '▁평양', '▁g', '95', '▁파일', '▁증거', '설을', '▁춘', '▁마쓰', '▁45', '▁신경', '▁1974', '▁185', '라엘', '일보', '붉은', '▁알아', '▁불과', '▁자체', '▁화학', '▁침공', '▁해방', '▁균', '구로', '▁만족', 'uc', '스케', '▁왔', '해의', '▁집단', '정으로', '▁구단', '▁잎', '받아', '트에', '▁레코', '되었지만', '면에서', '▁v', '▁주연으로', '▁길이', '▁예선', '▁간주', '98', '▁종종', '틀랜드', '▁사후', '▁발표했다', '▁나뉘', '▁게이', '▁소프트웨어', '다운', '▁경향', '▁요리', '▁라이브', '네시아', '▁건강', '▁델', '일러', '▁여행', '▁시내', '▁대립', '▁쇠', '용으로', '▁나폴', '우리', '▁샌', '▁기획', '민국', '개를', '사키', '▁좀', '무원', '티브', '▁이끄는', 'ice', '▁텍', '▁쪽', '▁이란', '▁개봉한', '▁쓰인다', '▁인간의', '하나', '하다고', '▁임명되었다', '약을', '레이션', '리스트', '▁에스', '▁무역', '▁편입', '▁죽은', '▁음력', '같은', '▁1976', '▁봄', '▁찰', '▁회원', '▁태평양', '었는데', '▁정상', '▁힘을', '류를', '▁컬', '▁나오는', '티나', '▁자신을', '학년', '▁짓', '▁폭발', '▁태어난', '▁때에는', '▁제임스', '▁받는', '▁방식으로', 'our', '▁엑', '▁인사', '자들을', '▁1978', '▁회복', '자에게', '▁개편', '▁비교적', '▁대형', '▁인기를', '▁수입', '▁한다는', 'se', '▁장애', '명한', '75', '▁대상', '후의', '▁설치되어', '▁속에', '▁뛰어난', '▁연맹', '하고자', '키아', '▁기계', '▁라틴', '리학', '▁사례', '▁둥', '▁공공', '▁조정', '박쥐', '▁자본', '▁수정', '▁국회의원', '적이다', '▁아름', '점에', '...', '었으나', '▁1920', '▁조선민주주의인민공화국', '▁&', '▁특수', '▁1971', '▁신설', '▁남성', '▁노력', '▁후손', '▁어떻', '다른', '▁했지만', '▁블루', '라비아', '▁메이저', '신이', '▁루이', '▁에피', '초등학교는', '음이의', '▁숲', '족의', '▁고양', '▁전차', '65', \"'(\", '데스', '32', '▁이것이', '▁저장', 'ip', '▁보이는', '▁형식', '▁위치한다', '종은', '▁사람의', '▁대규모', '화는', '▁고구려', '▁1968', '▁1963', '▁1975', '▁몽골', '▁180', '▁초등학교이다', '▁함수', '▁불가능', '▁카메', '▁피아', '▁복잡', '니어', '포르', '▁네트워크', '▁프로듀', '▁발견된다', '▁농업', 'av', '룬다', '▁이스라엘', '어져', '▁내에서', '형이', '▁e', '▁않을', '로나', '여자', '▁부터', '▁1930', '정책', '▁파리', '▁털', '동의', '▁한다고', '▁원칙', '▁문화재', '▁사실상', '료로', '▁우선', '▁초반', '▁흔히', '▁고전', '리트', '▁내용을', '▁시대에', '▁편성', '▁홈런', '▁식물', '▁본격', '소에서', '▁1940', '▁감염', '▁링', '이었고', '▁아르헨', '유럽', '차례', '43', '▁꿈', '싱턴', '▁있었으며', '▁콩', '▁발매되었다', '군으로', '▁과정을', '49', '▁조금', '▁부인', '종으로', '▁1969', '▁스튜디오', '▁외에', '국을', '▁쉬', '▁기억', '▁구성된', '▁인하여', '▁근대', 'ik', '오프', '▁나카', '▁북서', ',5', '▁트리', '▁300', '▁지역은', '구역', '▁속에서', '터테인', '▁팀은', '▁나서', '▁전혀', '셔널', '▁이전에', '▁어떻게', '미디', '▁모스크바', '제에', '▁원작', '시와', '▁참석', '료를', '품을', '▁아주', '▁도움을', '▁형태의', '▁유명하다', '▁통한', '▁윈도우', '▁있었으나', '예술', '▁열대', '▁구체', '▁아랍', '▁오리', '▁행사', '크라이나', '려는', '▁죽음', '▁잭', '지기', '분의', '▁대상으로', '아버', '▁잘못', '▁증명', 'for', '▁세이', '▁암살', '▁중화인민공화국', '▁설정', '▁사회적', '▁섭', '▁물러', '76', '▁때에', '▁하는데', '▁사용되었다', '전으로', '지면서', '28', '▁탄생', '▁본선', '▁확정', '▁플로', '드에서', '라크', '▁사는', '대회', '었지만', '▁1948', '▁Z', '▁정도의', '▁비난', '법의', '서에', '▁캄', '▁숫', '▁카운', '세계', '공업', '인들의', '▁등장하는', '어와', '▁공부', '▁했고', '▁시장', '▁창설', '년이', '▁짧은', '동음이의', '권에', '▁되면서', '어도', '▁개선', '▁박사', '▁현실', '▁철도역이다', '▁보급', '받은', '▁묻', '▁하였고', '▁기사', '▁한때', '권은', '▁뜻이', '라가', '법에', '인에', '▁', '이', '다', '.', '에', '의', '는', '로', ',', '하', '1', '을', '가', '고', '지', '서', '한', '은', '기', '0', ')', '(', '으', '2', '사', '대', '리', '시', '를', '년', '스', '도', '인', '일', '아', '자', '9', '어', '있', '라', '수', '나', '부', '그', '전', '되', '정', '국', '과', '해', '주', '었', '제', '들', '성', '장', '3', '구', '여', '상', 'e', '적', '동', '했', '5', '8', '월', '위', 'a', '와', '4', '원', '선', '게', '공', '였', '보', '6', '마', '만', '조', '트', '중', '며', 'o', '드', '7', '화', '경', '학', '미', '교', '소', '우', '유', 'r', '세', 'i', '비', '신', '오', '문', '치', '르', 'n', '명', '개', '면', '계', '역', '후', '연', '관', '등', '회', '된', '것', '진', '터', '영', '모', '용', '\"', 't', '발', '프', '군', 'l', '당', '무', '산', '러', '작', '재', '방', '민', '타', '니', '음', 'm', '데', '단', '거', 's', '레', '통', '간', '카', '분', '반', '생', '바', '안', '체', '포', '차', '행', '크', '내', '호', '현', '식', '운', '본', '노', 'u', \"'\", '남', '할', '종', '파', '물', '양', '요', '때', '출', '속', '임', '실', '야', '두', 'S', '-', '함', '형', '던', 'C', '결', '토', '강', '설', '루', '려', '표', 'A', 'c', '히', '키', '코', '각', '법', '초', '승', '독', '태', '업', '직', '건', '까', '디', '불', 'h', '래', '입', 'd', '배', '버', '력', '합', '록', '또', '점', '최', '예', '않', '번', '받', '매', '피', '같', '았', '약', '천', '른', '권', '왕', '말', '베', '메', '티', '《', '》', 'M', 'T', '심', '판', '격', '립', '알', '브', '청', '더', '금', '목', '감', 'B', '투', '추', '처', '북', '없', '항', 'f', '란', '달', '~', '평', '활', 'F', 'I', '외', '네', '열', '따', '린', '특', '저', '·', 'g', '집', '언', '편', 'P', '석', '준', '복', 'D', '테', '향', '즈', '병', '광', '류', '변', 'p', 'k', '질', '름', '령', '난', 'y', '족', '근', 'E', '랑', '급', '총', '람', '황', '송', '김', '존', '페', '축', '많', '올', '쪽', '능', '박', '술', '참', '철', '울', '론', '별', '살', 'L', '환', 'R', 'N', '온', '창', '악', '증', '품', '절', '및', 'O', '육', '료', '곡', '쿠', '백', ':', '새', '쓰', 'K', '플', '져', 'G', '순', '돌', '졌', '량', '필', '께', 'b', '확', '애', '망', '든', '머', '규', '림', '글', '패', '케', '색', '워', '팀', '범', '탈', '날', '견', '극', 'H', '쟁', '클', '련', '삼', '탄', '너', '친', '충', 'V', '랜', '째', '손', '봉', '막', '_', '책', '접', 'v', '뒤', '못', '완', '협', '퍼', '션', 'U', '/', '담', '런', '밀', '채', '츠', '길', '녀', '습', '골', '측', '블', '혼', '곳', '취', 'W', '럽', '엔', '험', '칭', '귀', '누', '겨', '암', '벌', '찰', '슬', '갈', '커', '씨', '’', '앙', '높', '웨', '슈', '‘', '첫', '뜻', '쳐', '침', '허', '즌', 'w', '죽', '효', '획', '볼', '널', '뉴', '응', '센', '났', '착', '큰', '락', '잡', '헌', '례', '희', '렸', '검', '폭', '득', '징', '움', '논', 'J', '%', '농', '섬', '걸', '캐', '홍', '럼', '틀', '풍', '느', '좌', '퇴', '택', '왔', '끝', '폴', '즉', '엘', '폐', '층', '될', '압', '줄', '픽', '헤', '덕', '윤', '싱', '맞', '먼', '념', '앞', '>', '찬', '맡', '긴', '<', '혁', '늘', '갖', '킨', '율', '써', '익', '〈', '〉', '턴', ']', '[', '릭', '잘', '램', '묘', '빈', '슨', '염', '링', '냈', '얼', '벨', '흥', '균', '억', '델', '닌', '앨', '객', '궁', '콘', '밝', '몇', '엄', '맹', '빌', '휘', '훈', '팔', '옥', '얻', '숙', '컵', '렌', '칸', '갔', '쇼', '“', '”', '텔', '칼', '됐', '므', '괴', '넘', '콜', '맥', '푸', '뷔', '좋', 'X', '몰', '붙', '덴', '핵', '찾', '혹', '낸', '흐', '둘', '빠', '떠', '켰', '템', 'x', '액', '혀', '샤', '컴', '릴', '몬', '뿐', '놓', '룹', '률', '쥐', 'z', '탑', '굴', '욕', '십', '끌', '떨', '힘', '잠', '롯', '싸', '죄', '칙', '갑', '핀', '벽', '척', '졸', '략', '춘', '풀', '몸', '먹', '눈', '맨', '켜', '잉', '끼', 'Y', '촌', '휴', '쳤', '잔', '컬', '홈', '벤', '칠', '렉', '밖', '大', '틴', '녹', '렇', '뛰', '몽', '뮤', '곤', '탁', '!', '륙', '폰', '털', '꾸', '혜', '멸', '탕', '둥', '겼', '밴', '웠', '솔', '겸', '혈', '돈', '딸', '톤', '렀', '혔', '셀', '쿄', '팅', '?', '낮', '넷', '컨', '홀', '앤', '님', '둔', '퓨', '냐', '」', '「', '윈', '텐', '랐', '겠', '욱', '롤', '톨', '탐', '떤', '헨', '꽃', '낙', '넓', '촉', '킬', '읍', '왜', '金', '셔', '콩', '렬', '붕', '山', '펜', '뇌', '랙', '옹', '덜', '슷', '흔', '융', '李', '옮', '섭', '=', '듬', '넣', '멘', '빛', '깨', '놀', '곧', '땅', '學', '킹', '닝', '룬', '헬', '팬', '딩', '쌍', '흑', '겐', '힌', '답', '켓', '챔', '롭', '룡', '값', '튜', '멤', '짓', '젤', '큐', '즘', '윌', '쉬', '쇄', '밤', '릉', '렵', '뢰', '웅', '닉', '흡', '튼', '젠', '씩', '납', '듀', '옛', '맺', '닥', '궤', '쉽', '짜', '랭', '숭', '듯', '+', '랫', '中', '文', '잃', '州', '숨', 'Z', '섯', 'j', '젝', '깊', '웃', '&', '롱', '빙', '꼬', '멜', '國', '멀', '쓴', '王', '캠', '펠', '웹', '핑', '큼', '잇', '즐', '끄', '텍', '믿', '뜨', '찍', '잎', '셰', '덤', ';', 'Q', '』', '『', '밍', '펼', '뷰', '짐', '틱', '빅', '돼', '뀌', '멕', '빨', '엽', '틸', '쇠', '짧', '삭', '곱', '子', '롬', '|', '校', '城', '첨', '폼', '東', '힐', '촬', '럴', '럭', '릿', '혐', '三', '道', 'q', '켈', '캘', '썼', '겪', '安', '곽', '낭', '랍', '딘', '옆', '밑', '켄', '氏', '쌓', '삶', '콤', '高', '흘', '人', '南', '벗', '뉘', '싶', '헝', '첩', '낳', '컷', '젊', '닐', '꿈', '냥', '빼', '옷', '랄', '天', '왼', '듣', '됨', '公', '얀', '읽', '닛', '톱', '냉', '셜', '°', '믹', '룩', '平', '寺', '엑', '픈', '事', '法', '벡', '낼', '섰', '숲', '퀴', '봄', '뒷', '엇', '띠', '書', '흰', '宗', '軍', '묵', '벼', '셋', '뤄', '튀', '븐', '붉', '잭', '쿨', '캄', '川', '元', '좀', '앗', '괄', '샌', '뿌', '義', '正', '끊', '뱅', '슴', '맛', '덮', '一', '씬', '넬', '팽', '녕', '륜', '찌', '–', '떻', '숫', '댄', '뱀', '府', '늬', '묻', '院', '長', '첼', '팩', '∼', '・', '짝', '둑', '웰', '石', '춤', 'é', '늄', '成', '쾌', '렴', '톰', '明', '쓸', '海', '셈', '탱', '뤼', '앵', '캔', '엠', '꼽', '깔', '陽', '뽑', '샘', '늦', '렐', '훨', '꺼', '밥', '닫', '西', '쫓', '光', '옌', '횡', '꼭', '君', '上', '太', '앉', '使', '郡', '팝', '德', '會', '原', '뼈', '륨', '地', '덩', '흉', '좁', '끈', '北', '겔', '武', '世', '끔', '家', '行', '部', '新', '봇', '렘', '本', '훗', '主', '꾼', '룸', 'ᆞ', '等', '깃', '룰', '뚜', '日', '都', '멍', '田', '윗', '神', '水', '韓', '덧', '춰', '펙', '랩', '門', '仁', '햄', '갤', '縣', '홋', '政', '퀘', '펴', '듭', '딕', '딜', '삽', '쿼', '꺾', '*', '龍', '生', '묶', '싼', '궐', '官', '륭', '經', '司', '탠', '퀸', '뮌', '윙', '無', '쌀', '꼴', '섞', '朴', '뉜', '겹', '里', '빗', '킴', '興', '눌', '훌', '띄', '趙', '×', '林', '긍', '첸', '슐', '펄', '洞', '忠', '녁', '밭', '셉', '깥', '돔', '相', '넌', 'а', '敎', '녔', '넥', '漢', '夫', '툰', '렛', '白', '民', '所', '永', '江', '聖', '낌', '五', '鄭', '펑', '臣', '和', '慶', '理', '빵', '돕', '馬', '깝', '史', '뿔', '張', '엉', '늑', '섹', '곰', '펀', '똑', '닷', '틈', '훼', '→', '全', 'の', '랴', '딱', '넨', '皇', '善', '컫', '前', '昌', '記', 'о', '흙', '힙', '朝', '굳', '將', '島', '帝', '툴', '갱', '定', '性', '핸', '心', '羅', '팡', '知', '尹', '化', '킥', '셸', '콰', '堂', '빚', 'и', '킷', '톡', '有', '겁', '껍', '之', '士', '풋', '小', '面', '맷', '닭', '벳', '崔', '十', '方', '겉', '京', '河', '松', '社', '쏘', '同', '뮬', '룽', '佛', '下', '時', '不', '淸', '굽', '멈', '古', '콥', '콕', '宮', '옵', '콧', 'α', '뇨', '봐', '푼', '通', '뻗', '代', '밸', '핫', '턱', '四', '信', '師', '딴', '럿', '年', '二', '떼', '꿀', '굿', '像', '렷', '탓', '퉁', '쯤', '論', '華', '죠', '後', '守', '닮', '劉', '樂', '쑤', '홉', '禮', '月', '祖', '八', '왈', '孝', '샹', '尙', '캡', '女', '굉', '줬', '權', '릎', '눅', '릇', '重', '圖', '쁜', '市', '붓', '뚫', '木', '字', 'е', 'н', '툼', '洪', '基', '칩', '黃', '싫', '낱', '物', '샬', '孫', '댐', '雲', '셨', '內', '陵', '놈', '立', '엣', '名', '業', '띤', '닿', '걷', '寧', '侯', '鎭', '듈', '吉', '낀', '合', '自', '科', '、', '廣', '驛', '헐', 'р', '삿', '判', '兵', '集', '갓', '흠', '派', '젖', '功', '殿', '柳', '順', '分', '봤', '監', '냄', '쩌', '衛', '宋', '承', '體', 'ー', '靑', '솟', '六', '春', '펌', '잊', '谷', '웬', '걱', '九', '周', '建', '議', '퐁', '景', '쉐', '※', '曹', '덟', '앱', '直', '옴', '솜', '•', '制', '돋', '곁', '美', '흩', '쥬', '쿤', '度', '톈', '둠', '슘', '者', '얄', '얇', '郞', '源', '굵', '英', '路', '륵', '긋', '쏟', '村', '얘', '福', 'á', '휩', '껴', '館', '멋', '짙', '宣', '見', '身', '左', '章', '萬', '팜', '탤', '花', '星', '貞', '꾀', '觀', '號', '렁', '#', '맵', '眞', '的', '$', '初', '節', 'с', '쯔', '밟', '濟', '꿔', '動', '治', '野', '훔', '덱', '泰', '뭉', '侍', '康', '副', '컸', '傳', '造', '챌', '캉', '왓', '뚝', '開', '根', '뤘', '外', '典', '音', '国', '壽', '떡', '令', '臺', 'к', '式', '쁘', '옐', '寶', '玉', '끓', '別', '應', '憲', '利', '햇', '{', '吳', '族', '奉', '思', '휠', 'い', '鮮', 'в', '뀐', '텀', '핍', '댓', 'ü', '后', '實', '命', '戰', '`', '씀', '語', '짱', '良', '御', 'ン', '唐', '弘', '修', '꽤', '保', '申', '촨', '잦', '맘', '횟', '智', '슭', '說', '浦', '親', 'л', '在', '作', '賢', '空', '錄', '阿', '픔', '形', '줌', '펫', '來', '塔', '省', '梁', '썬', '빔', '進', '位', '괘', '土', '敬', '앓', 'т', '老', '詩', '싯', '歌', '뒀', '벅', '밋', '工', '色', '界', '津', 'ا', '徐', '붐', '右', '엿', 'ο', '線', '參', '井', '秀', '碑', '갇', '얹', '땄', '눠', '七', '流', '延', '溪', '齋', '言', '風', '陳', '多', '깎', '딪', '護', '運', '멧', '샨', '間', '헥', '땃', '줘', '用', '팟', '如', '數', '藤', 'ö', '맑', '맏', 'ó', '常', 'ā', '藏', '口', '찐', 'μ', '슛', '벵', '玄', '任', '達', '꼈', '反', '텝', '楊', '몫', '許', '닦', '찔', '密', '甲', '場', '걀', '先', '鳳', '車', '옳', '對', '志', '깐', 'í', '亭', '뭇', '곶', '씌', '錫', '沈', '끗', '靈', '낫', '朱', '郎', '頭', '윅', '統', '쟈', '≪', '印', '≫', '夏', '禪', '얽', '佐', '랏', '熙', '麗', '烈', '姜', '팍', '出', '츄', '務', '뻔', '察', '團', '伯', '園', 'ν', '愛', '百', '識', '쭉', '榮', '器', '草', '關', '캅', '豊', '}', 'ι', '湖', '員', 'ς', '嘉', '現', '鄕', '튬', '儀', '귄', '諸', '뚱', '勝', '橋', '從', '意', '植', '晉', '商', '解', '墓', '■', '낚', '氣', '볍', '앰', '遠', 'ä', '祭', '鐵', '뜬', '○', '益', '千', '뱃', '容', '管', '뻐', '헛', '加', '竹', '伊', '起', '領', '坐', '챙', '쾰', '궈', '沙', '遺', '第', '팰', '찮', '調', '엮', '귤', '魏', '엥', '쩔', '恩', '秦', '處', '想', '機', '싹', '욘', '力', '非', 'ρ', '居', '雄', '―', 'し', '뾰', '報', '異', '畵', '핏', '렝', '뫼', '꿨', '泉', '惠', '次', 'ん', '캣', '柱', '鍾', '샐', '術', '黨', '火', 'β', '始', '我', 'τ', '支', '훙', '岩', '。', '崇', '뭄', '丁', '탬', '紀', '室', '쉴', '秋', '俊', '受', '得', '宇', '盧', '섀', 'か', '샀', '品', '웍', '廳', '昭', 'な', '資', '池', '깜', '類', '삐', '푹', 'う', '交', '얕', '낄', '넛', '果', '랬', '쿡', '싣', '舍', '뮈', '嚴', '營', '咸', '目', '育', '提', '돗', '꽂', '姓', '챈', 'ل', '曲', '늙', '系', '뜸', '局', '試', '總', 'ε', '믈', 'λ', '仲', '렙', '妃', '齊', '香', '@', '彦', '富', '卿', '圓', '尉', '少', 'ス', '顯', '能', '手', '波', '男', '表', '魚', '致', '꽝', '껏', '發', '댈', 'м', '群', '瑞', '냅', '慧', 'д', '吏', '숀', '戶', '防', '極', '角', '눔', '갚', '象', '農', '셴', 'た', '共', '母', '큘', '恭', '츰', '産', '情', '首', '쾨', '故', '署', '늪', '薩', '»', '촛', '짖', '邑', '즙', 'σ', 'と', '因', '췄', '蘇', '샵', '썩', '훤', '博', '☆', '짚', '尊', '樞', '탭', '虎', '覺', '陸', '찢', '△', '庵', '仙', 'й', '町', '結', '峰', '牧', '電', '갯', '듐', '쪼', '入', '房', '릅', 'イ', '몹', 'ル', '딧', '陰', '區', 'у', '半', '펭', '뿜', '獻', '蓮', '췌', '協', '倉', '屬', '淑', '閣', '녜', '빽', '完', '뜰', 'è', '域', '蔡', '뇽', '巖', '贊', '딥', '룻', '板', '겟', '範', '쁨', '슌', '괌', '굶', '溫', '翼', '菩', 'り', '普', '呂', '씻', '靖', '숍', '變', '～', '펩', '勳', '可', '쩐', '깁', '父', '넉', '乙', '連', '−', '쿰', 'る', '鶴', '맬', '哲', '엎', '銀', '丹', '퀄', '퀼', '샷', '邊', '僧', '死', '督', '뮐', '엡', '律', '燕', '핼', '学', '貴', '壇', '«', '船', '越', '祠', '台', '友', '淵', '뭐', '珍', '復', 'κ', '衆', '彌', '淳', '職', '載', '輔', '久', '벙', '—', '斗', '탔', '볶', '舊', '港', '롄', '\\\\', '藝', '近', '郭', '샴', '슝', 'ラ', '젯', '喜', '境', '莊', '쵸', '隆', '種', '赤', '奇', '잼', '껑', '벚', 'に', '具', '똥', '꿰', '뜯', '足', '윽', '允', '精', '臨', '썰', '衣', '땀', '澤', '純', '舞', '層', '特', '禹', '잣', '튠', '毛', '禁', '座', 'ま', '煥', '퓌', '球', '둡', '촘', '爲', '둬', '亞', '惡', '旨', '龜', '′', '素', '訓', '均', '맴', 'ら', '住', '낡', '뵈', '뽀', '촐', '陀', '雨', '奎', '쥘', '址', '廟', '洋', '멩', '쑨', 'リ', '儒', '꼼', '洙', '養', '劇', '條', '誠', '期', '轉', '텅', 'ト', '깅', '盛', '蘭', '閔', '魯', 'き', '備', '射', '隨', '괜', '멱', '浩', '私', '욤', '댕', '慈', '낯', '퓰', 'π', '聲', '丘', '内', '執', '€', '牛', '警', '좡', '今', '隊', '刑', '葉', '藩', '要', '念', '隱', '醫', '켐', '那', '放', '젓', '쥔', '以', '麻', '緣', '取', '킵', '列', '岳', '弼', '兼', 'γ', '淨', '뎀', '됩', '綱', '츨', '獨', '팎', '胡', '布', '質', '兒', '岡', '戦', '至', '桂', '聯', '팻', '±', '愼', '持', '謙', '훅', '檢', '改', '穆', '雙', 'は', '祿', '肅', '靜', '量', '캇', '觸', '羽', '製', '嶺', '希', '然', '若', '밧', '冠', 'ッ', '딤', 'く', '狀', '茂', '밈', '亂', 'ن', '封', '譜', '웜', '쉰', '圭', '洛', '由', 'お', '캥', '戒', '苦', '몄', '離', '띈', '釋', '쏠', '比', '錦', '約', '計', '랠', '聞', '助', '惱', '望', '賀', '썸', '퀀', '夢', '補', '銅', '于', '汝', 'я', 'ي', 'ク', '亨', '孔', '件', '飛', '珠', '뇰', '單', '甫', 'さ', '팥', 'δ', '帶', '卷', '欲', '兩', '委', '裵', '쑹', '칫', '剛', '末', '鏡', '뱌', 'ア', '摠', '考', '藥', '엌', '‧', '^', '왁', '客', '寅', '餘', '孟', '敏', '辛', '問', '屋', '樹', '杜', '易', '清', '짠', '尾', '恒', '鳥', '型', '留', 'à', '眼', '準', 'ī', 'て', '向', '誌', '쏜', '챠', '慕', '헴', 'η', '夷', '廷', '樓', '쐐', 'ç', '宰', '麟', '뭔', '퍽', '克', '役', '祥', '端', '윔', '쿵', '꽁', '威', '紅', '格', '滿', '環', '갸', '폈', '丞', '求', '黑', '솥', '技', '影', '会', '梅', '好', '晋', '葛', '釜', '鉉', 'г', '烏', '宅', '僉', '翰', 'あ', '究', '細', 'ь', '食', '쨌', 'ر', 'م', '厚', '己', 'و', '演', '叔', '辰', '摩', '넴', '乘', '講', '丙', '諫', '먀', '秉', '酒', '덥', '衡', '잖', '回', '啓', '選', '題', '騎', '깡', '뺏', '指', '曺', '證', '財', '乾', 'ち', '則', '쩍', '徳', '筆', '돛', '煩', '遊', 'É', 'â', 'タ', '增', '斷', '邱', '簡', '茶', '滅', '妙', '祐', '骨', '服', '話', '炳', '假', '籍', '敦', '未', '雅', '욜', '懿', '桓', '略', '젬', '句', '댁', 'こ', 'だ', '散', '宜', '条', '點', '콴', '昇', 'С', '邪', '늠', '雜', '裕', '딛', 'み', '軒', '銘', '얌', '扶', '輪', 'ы', '눕', '央', 'ú', '遼', '巨', '續', '革', '歸', 'シ', '뛴', '設', '刺', '襄', '鎬', '툭', '坊', '旗', '雪', '季', '擧', '硏', '策', '登', '盟', '뎅', '賜', '찻', 'つ', '休', '振', '曆', '丸', '征', '倭', '庶', '難', '칵', '震', 'ド', '庭', '繼', '固', '寬', '殷', '핌', '積', '廉', 'ч', '依', '契', '庫', '翁', '쑥', '貪', '卞', '勞', '病', '뮴', 'θ', 'が', '健', '놉', '앳', 'ί', 'ᄀ', 'を', '罪', '送', '읊', '簿', '峴', '最', '紙', '웁', '́', '俗', '旌', '感', '願', 'ω', '株', '欽', '版', '超', 'п', '洲', '옅', '姬', '贈', '際', '뺨', '힉', '刻', '坡', '寫', '皮', '⁄', '注', '童', '땐', '볕', '캬', 'も', '夜', '消', '燮', '뛸', 'υ', '詞', '賓', '邦', '★', '紫', 'ά', '翊', '쉘', '爾', '琴', '뺀', '懷', '漏', '僕', '再', '勢', '逸', 'す', '梵', '룀', 'カ', '晩', 'マ', '旅', '組', 'ᄋ', '区', '匡', '潤', '蒙', '諦', '횃', 'ό', '徒', '盤', '鑑', 'Δ', '←', '程', '蓋', '追', '介', '팸', 'れ', '深', '鼎', 'б', '序', 'え', '甘', '速', '過', '陶', 'よ', '歷', '썹', '쭈', '切', '鐘', '∙', '才', '県', '亮', '惟', '納', '엷', '磨', '巡', '站', '뀔', '봅', '料', '뽕', '凉', '止', '症', '퀵', 'د', '譯', '掌', '殺', '浮', '옙', '構', '팁', 'ジ', 'ロ', '配', '落', '壤', '芳', '샛', '推', '楚', '웸', 'š', 'з', '存', '毅', '刊', '熊', '꼰', '斯', '磁', '勇', 'ñ', '習', '背', 'ł', '꿇', '텃', '畿', '碩', '編', '置', '蘊', '갠', '숄', '尼', '虛', '於', '灣', '露', '땡', '춧', '泳', 'х', 'レ', '宿', '戸', '넵', '施', '頼', 'ê', '也', '而', '袁', '奴', '疏', '染', '維', '述', '活', '織', '씹', 'キ', '텡', '嬪', '街', '깬', '쉼', 'け', '項', 'フ', '率', '鹿', '失', '岐', '켤', 'ب', '祚', '迦', '냇', '뎌', '評', '깼', '奏', '當', '짤', '튿', '勒', '将', '崎', '뻘', '壁', '峯', '稅', '鳴', 'К', '輝', '差', '帥', '熱', '真', '〜', '專', '弟', '與', '告', '墳', '鬼', 'ß', '긁', '何', '干', '幸', '揚', '導', '米', '坪', '潭', '睦', 'ニ', '倫', '規', '墨', '爭', '步', 'コ', '堅', '模', '階', '刀', '援', '꽉', '찼', '휼', 'オ', '標', '닙', '뱉', '펨', 'ᄂ', 'っ', '冬', '舜', 'А', '移', '貢', '徽', '血', '賊', '蹟', '쇤', '撰', '獄', '紋', '綠', '引', '壬', '燈', '耶', '댑', '享', '決', '錢', '텟', '폄', '幕', '渡', '謝', '꺽', '눗', 'ᄅ', '案', '兪', '眠', '視', '貫', '깍', 'じ', '害', '蔚', '랗', '윳', '홑', '吾', '耳', '賞', '鼓', '其', '嗣', '葬', 'ô', '午', '奈', '弓', '愚', '照', '邉', 'ᄃ', '勤', '般', '輸', '撫', '青', '줍', '紹', '裝', '▲', '阮', '段', '솝', 'で', '秘', '胤', '辭', '힝', 'ᄆ', '豆', '솅', 'æ', 'ı', '候', '坂', '讀', '限', '얏', '戱', '浪', '伽', '庚', '歲', '쏴', '伏', '됭', '젼', '函', '操', '教', 'ナ', '栗', 'ū', 'φ', 'М', '豫', '査', '降', '웡', '喪', '魔', '算', '쌈', '쭤', '附', '뢴', '劍', '篇', '鴻', 'ə', 'や', '堤', '微', '忍', '授', '쎄', 'わ', '兄', '喆', '齡', '챕', 'ᅳ', '帖', '芝', '薛', '쫒', '諡', '£', 'П', '関', '頂', '符', '륀', 'ウ', '뎃', '創', '禧', '뜩', '傅', '壯', '材', '庄', '稷', '破', '鬪', '춥', '曾', '顔', '唱', '塚', '睿', '瞋', '障', '빴', '是', '班', '韻', '뎬', '幀', '級', '惑', '映', '蜀', '衍', '哀', '肉', '号', '廢', '彩', '接', '赫', '寒', 'س', '優', '閭', '뵤', '쩡', 'テ', '收', '脈', 'ø', 'ć', '伐', '脫', '舌', '還', '驪', '놨', '森', '檀', '須', 'В', 'ノ', 'ミ', '淮', '董', '郵', 'ш', '─', '冊', '꿩', '텁', 'バ', '箕', '萊', '談', '⟫', 'ャ', '徵', '油', '땜', '숱', '佑', '攝', '経', '묀', '찹', '岸', '救', '牙', '鍊', '̊', '包', '桃', '菴', '巴', '癡', '碧', '展', '請', '맙', '헵', '⟪', '味', '尺', '孤', '幹', '敵', '꿉', '튕', '給', '齒', '씽', '宝', '漁', '稱', '終', '讓', '退', '匠', '卯', '舟', '蔵', '黎', '놔', 'エ', 'チ', '融', '랸', 'ã', '去', '婦', '疑', '댜', '륄', '젱', '찜', 'サ', '逆', '떴', '卓', '潘', '볜', '헹', 'ع', '梨', '뤽', '뽐', '亥', '絶', '跋', '넋', '좇', 'έ', '寂', '잿', '橫', '番', '雍', '例', '崖', '換', '駐', '鷄', 'ひ', '唯', '実', '必', '陣', '빡', '쌌', '審', '蕭', 'ت', '羊', '숯', '컹', '仕', '湯', 'モ', '屯', '默', '县', '強', '腹', '썽', 'ュ', '價', '示', '옻', '잰', '햐', 'ᅵ', '柴', '旭', '覆', '賦', '鹽', '셍', '巫', '彭', '酉', '〕', '輿', '얗', '了', '婆', '戊', '毘', '菊', '著', '귈', '묄', '便', '培', '愍', '胎', '꿋', 'ィ', '慢', '爵', '뻣', '숏', '他', '낵', '潮', '탸', 'ē', '俱', '祀', '껌', 'ō', '〔', '댔', '볏', '앎', '흄', '貨', '푀', 'め', '築', '航', 'Б', 'י', 'グ', '妻', '朔', '澄', '禎', '遂', '酸', '丈', 'χ', 'ц', '巳', '濬', 'Р', '砲', '쌔', '広', '뷸', '녘', '括', '泥', '牟', '굼', '밌', '븀', '핥', 'Đ', '□', '停', '打', '折', '癸', '蔣', '覽', 'ư', 'ば', '牌', '탯', 'ょ', '輕', 'ズ', 'ム', '片', '헷', '底', '及', '듦', '줏', 'ه', '仇', '複', '軌', '툇', '占', '幡', '莫', '衙', '쳇', '万', '稿', 'ハ', '楽', 'プ', '祝', 'ή', '拳', '沃', '着', '蟲', '鎌', '炎', '탉', '戴', '整', '泗', 'ガ', '裁', '똘', 'メ', '肖', '駿', '챗', '券', '尚', '짊', '\\xad', 'ブ', '拓', '殊', 'ă', 'ˈ', 'ᄉ', '升', '召', '嫡', '庸', '槐', '沢', '突', '筵', '討', '乳', '擊', '菜', '휜', '佳', '尋', '店', '桐', '놋', '彰', '杉', '梧', '短', '쨩', '禍', '范', '鼻', '測', '曉', '朗', '枝', '鎮', '除', '덫', '坤', '就', '敍', 'Α', 'ろ', '係', '抗', '遷', '갬', 'ë', 'ː', '災', 'ᄇ', 'ゃ', '傑', '勅', '奭', '彈', '戌', '淡', '炯', '訴', 'И', '宙', '彫', '訪', '註', '냑', 'ύ', '凡', 'Н', '丑', '競', '賈', '辨', '溝', '藍', '幢', '態', '早', '갭', 'า', '胞', '荷', '被', '悲', '抄', '暴', '漆', '虞', 'ح', '壺', '槃', '汗', '耆', '額', '●', '穴', '雷', 'デ', '昊', '渾', '笠', 'Π', '乱', '昧', '苑', '荊', '待', '氷', '豪', '履', '甁', '遍', '郷', '魂', 'č', '乃', '禦', '綜', '綾', '艦', '閑', '隋', '괭', '눙', '욧', '仰', '幽', '探', '核', '穀', '卜', '各', '婚', '鈴', '塘', '毒', '炭', '粉', '鉢', '췬', 'Г', '∞', '垂', '扈', '賴', '껀', '젭', 'Σ', '犯', '遞', '拜', '눴', '沖', '珪', '采', '귓', '亡', '側', '呼', '権', '沼', '砂', '賣', '넙', 'ş', '█', '悟', '捨', '涅', '租', '頌', '앴', '첵', 'ふ', '濃', '荒', '費', '遇', '닳', 'ו', '儉', '迎', '뉠', '쁠', '쿱', '悅', '熟', '蕃', '궂', '쭝', 'å', '云', '企', '浚', '薰', '넜', 'Á', 'ж', '་', '投', '来', '耕', 'Т', '為', '総', '肥', '陜', '뺄', 'ゆ', '伴', '供', '宴', '按', '絲', '謹', '走', '긱', 'ぶ', '応', '卽', '庾', '效', '曰', '跡', 'そ', '卒', '哈', '浜', '鏞', '튤', '懸', '矢', '認', '뷜', 'ダ', '余', '卑', '寛', '床', '排', '擇', '晴', '璋', '網', 'ビ', '低', '急', '暗', '更', '盡', '黄', '넝', '숴', 'ق', '姫', '屠', '뵐', 'ی', '幼', '杖', '樣', '驗', '늉', '뚤', '콸', '億', '謀', '軸', '陟', '깟', '伝', '僞', '筑', '縛', '諱', '쥰', 'パ', '席', '攻', '晶', '섣', 'Д', 'ה', 'ف', 'せ', 'ど', 'ワ', '倍', '宏', '息', '窟', '鎔', '兆', '堯', '菌', '諭', 'ᄌ', '勿', '灘', '鄧', '헉', '冷', '彬', '獸', '舒', '馮', '喬', '忘', '眉', '緖', '豐', '飯', '쳅', '팹', '免', '含', '播', '洗', '燦', '瓦', '꾹', '垣', '奮', '招', '瀬', '謨', '겜', '†', 'ず', '圈', '烽', '迪', 'Л', 'ة', '銃', '쟝', '짬', 'ᄒ', '又', '戎', '緯', '훠', 'ᄏ', '塾', '駕', '鷹', '뎠', '솽', '펍', '【', '】', '束', '答', '買', '頓', '髮', '돤', 'ר', '壓', '奥', '蔭', '衝', '뭘', 'ج', 'ツ', '呪', '課', '頃', 'Κ', '党', '冥', '慰', '負', '껄', '벰', 'Ö', '↔', '憂', '昆', '텨', 'О', '浙', '蒲', '詳', '윕', '东', '充', '損', '뀜', '몐', '셤', '왠', 'ョ', '忽', '扇', '蜜', '劑', '强', '架', '索', '聚', '黒', '涉', '混', 'ร', '体', '渠', '犬', '篆', '險', '뗀', 'ð', 'ơ', 'ェ', '荀', '阪', '寄', '悌', '掛', '湘', '爐', '辦', 'ŋ', 'ほ', 'セ', '押', '詔', '괵', '듄', '셧', '숟', 'Φ', '沿', '綏', '韋', '뗏', '쬐', '팠', 'ï', 'ケ', '偏', '寇', '往', '揮', '樑', '訥', '阜', '샥', '텼', '⋅', 'び', '迷', '雀', '믐', '뷴', '쫄', 'î', 'ý', '瞻', '鑄', '빤', '푈', '伸', '嶽', '潛', '灌', '疾', '筒', '羲', '겅', '떳', '偉', '対', '戚', '拉', '糖', '訟', '顧', '맣', '왑', '此', '澈', '箭', '켠', 'ò', 'ệ', '付', '忌', '桑', '絃', '챘', '堀', '帽', '悼', '沒', '甄', '笑', '鉄', '镇', '雕', '닻', '씰', 'む', '傷', '屛', '幾', '溟', '練', '襲', '왐', '参', '綿', '諺', '適', '只', '姚', '晦', '満', '쏙', '챤', '≤', 'ね', '乞', '柔', '栢', '樊', '蒼', '観', '鑛', '끽', '숑', 'ś', 'א', '々', '꼿', 'ヶ', '갉', '뎁', '似', '广', '捕', '橘', '覇', '鵬', 'Ε', 'أ', 'ボ', '堡', '蘆', '蛇', '쪄', '刹', '郁', 'ž', '廊', '浅', '繫', '耽', '샅', '햅', '吹', '渓', '僖', '娘', '巾', '斤', '燧', '畫', '療', '线', '貝', '遮', '隣', '霞', 'э', 'ю', 'น', '冀', '脚', '誤', '꿍', '쥴', '톳', 'Å', 'ァ', '塵', '已', '看', '祉', '綬', '訣', '違', '넹', '뺑', '뻑', 'ᅡ', '旦', '游', '纂', '芸', '짼', '쳄', '휨', 'Š', 'ա', 'げ', '慮', '琉', '翔', 'ネ', '値', '劃', '斥', '暦', '瑜', '膺', 'ù', '借', '勸', '吐', '峻', '曜', '減', '燁', '皆', '窯', '膜', '零', '従', '循', '晟', '狄', '猛', '璿', '禄', '互', '圃', '幻', '燒', '獅', '瓊', '盜', '萱', '讚', '뭍', '債', '婢', '裏', '譚', '貿', '鳩', 'ế', '円', '屈', '歐', '漫', '皐', '趣', '鐸', '岑', '旋', '갛', 'ش', 'ヤ', '勉', '婁', '杏', '杓', '汀', '煙', '礙', '稽', '竿', '賤', '遣', '馨', '割', '卵', '塞', '変', '寵', '怪', '斬', '横', '狂', '珉', '避', '땔', '묽', '짰', '칡', 'ベ', '揆', '敞', '榜', '琳', '盆', '翠', '闕', '넸', '्', '恪', '挺', '擬', '旺', '棋', '睡', '翟', '裴', 'へ', 'ゅ', '亀', '抱', '湛', '粹', '솁', '쉔', '힛', 'ì', 'ń', '伎', '弐', '衿', '껫', '톄', 'ψ', 'ك', 'र', 'ソ', '姑', '弁', '怡', '昔', '杞', '畢', '確', '礪', '祈', '肇', '趾', '週', '預', 'ğ', 'Λ', 'ё', '誘', '閤', '륌', '폿', '徹', '殉', '遵', '邵', '閉', '飾', '빳', '쩨', '쳉', 'Ω', 'َ', '凝', '媛', '恐', '斎', '票', '辯', '閻', '켁', '斜', '熹', '画', '畜', '縱', '藻', '遜', '냘', '횔', 'ф', '個', '倒', '寿', '廻', '析', '沛', '炫', '窩', '腦', '説', '龐', '냔', 'ě', '♪', '串', '廬', '恵', '涼', '濱', '繁', '纏', '邸', '욥', '잽', '튈', 'ز', 'ᄎ', 'ᅩ', 'ḥ', '叢', '妄', '帳', '湜', '盈', '祗', '羌', '聰', '肩', '駒', '뒬', 'Μ', 'ξ', '„', '竺', '腸', '託', '録', '®', 'У', 'ב', '卦', '壞', '拔', '杭', '渭', '糧', '脂', '謁', '퉈', 'ạ', 'ピ', '奧', '峽', '快', '柏', '漸', '隸', '顕', '섐', '탰', 'ᄑ', '妓', '朋', '札', '棟', '檜', '渤', '珥', '畠', '秩', '詠', '鴨', '빻', 'Ü', 'і', '‚', 'ヒ', '凰', '峙', '灰', '猿', '瑩', '繡', '脩', '詮', '謠', '铁', 'ぎ', '圍', '崗', '敷', '溶', '脣', '苻', '醴', '隅', '좨', '쭐', 'מ', '场', '幣', '弥', '捷', '皓', '績', '誓', '陝', '녓', '씁', '윷', '죤', 'Γ', 'ご', '否', '彼', '悔', '滉', '狗', '窮', '褒', '輯', '鈞', '鎖', '뢸', 'ा', '什', '叉', '棺', '牒', '猶', '逢', '鄒', '銓', '閱', '▶', 'ぬ', '倂', '冶', '凶', '患', '液', '瀛', '疇', '筋', '薄', '裂', '阳', '냠', '찧', 'ζ', 'ṭ', 'ễ', '夕', '尤', '拘', '柄', '璽', '需', '響', '뗄', '∎', '勃', '戀', '楷', '歡', '溥', '軾', '鎰', '駅', '윰', '予', '伺', '採', '紗', '詹', '針', '飮', '黔', '깰', '멎', '틋', 'ܐ', '寸', '崙', '拾', '獲', '祇', '禿', '蠶', '遲', '鋼', '飡', '亦', '滋', '疫', '硬', '꽈', '뵙', '셩', '웩', '츤', 'ל', 'ა', 'ザ', 'ポ', '吸', '巢', '恨', '據', '暉', '楞', '电', '畏', '誕', 'ʼ', 'ก', 'ᄐ', '僚', '嵌', '椒', '爆', '瓚', '研', '篤', '胄', '螺', 'ʿ', 'ъ', '倻', '償', '哉', '爀', '狼', '窓', 'ง', '与', '你', '凌', '危', '坦', '塑', '弗', '弱', '械', '棲', '澗', '誼', '뜀', '솀', 'ɪ', 'Ο', 'อ', '♭', 'ぐ', '滄', '苗', '趺', '밉', '뺐', '‰', 'ゴ', '冕', '団', '旻', '瑛', '穡', '辺', '잴', '쫑', 'ώ', 'ᅮ', '偶', '厭', '当', '殘', '滑', '瓜', '竟', '臧', '醉', '隴', '믄', '웽', '쿈', '훑', 'Β', '図', '幅', '恋', '愧', '暎', '槍', '粒', '縮', '臥', '虹', '衰', '醮', '霖', '퍄', '§', 'Č', 'đ', 'ʻ', 'Ф', 'ი', 'ṣ', 'ギ', '凱', '叱', '幷', '批', '晃', '栄', '每', '気', '琦', '禾', '諍', '辟', '鞍', '韶', '驅', '쐈', '쾡', '홰', 'Θ', 'ม', 'ホ', '兎', '峨', '曼', '沆', '瀑', '燃', '罰', '聽', '腫', '腺', '莞', '鱗', '뷧', 'ط', '丕', '兢', '到', '媒', '悳', '掾', '旬', '曇', '汚', '涵', '牡', '猫', '璧', '郊', '髻', '鬱', '龙', 'З', 'べ', 'ヴ', '墩', '拍', '敗', '旣', '椿', '沔', '点', '籠', '継', '耀', '豹', '貸', '閩', '鞠', '쇳', '쟌', '刷', '哥', '懺', '昕', '楓', '痛', '矣', '礎', '竇', '虜', '鄴', '겊', '꽌', '垈', '尸', '慚', '楠', '絹']\n"
     ]
    }
   ],
   "source": [
    "#Q. 특수 token 7개를 제외한 나머지 token들을 출력해봅시다.\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (1) MASK 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []  # 각 단어별 토큰 인덱스 그룹을 저장하는 리스트\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        # 현재 토큰이 단어의 연속 토큰(즉, u\"\\u2581\"가 없으면)인 경우, 마지막 그룹에 추가\n",
    "        if len(cand_idx) > 0 and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask 진행: 단어 단위로 masking\n",
    "    mask_lms = []  # mask된 토큰의 인덱스와 원래 값을 저장할 리스트\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값 생성\n",
    "        \n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:\n",
    "                # 80% 확률: [MASK]로 대체\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9:\n",
    "                # 10% 확률: 원래 토큰 유지\n",
    "                masked_token = tokens[index]\n",
    "            else:\n",
    "                # 10% 확률: 무작위 토큰 대체\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            \n",
    "            # mask된 토큰 정보를 저장\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            # 실제 토큰을 masked_token으로 변경\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (2) NSP pair 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 리스트의 길이: 17\n",
      "가장 짧은 리스트의 길이: 9\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(line) for line in doc)  # 가장 긴 리스트의 길이\n",
    "min_len = min(len(line) for line in doc)  # 가장 짧은 리스트의 길이\n",
    "\n",
    "print(f\"가장 긴 리스트의 길이: {max_len}\")\n",
    "print(f\"가장 짧은 리스트의 길이: {min_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])\n",
    "        current_length += len(doc[i])\n",
    "        \n",
    "        # 현재 chunk에 최소 2개 이상의 문장이 있고, 마지막 문장이거나 max_seq 이상의 길이가 되면 처리\n",
    "        if len(current_chunk) > 1 and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # tokens_a와 tokens_b를 생성: current_chunk를 적절히 분할합니다.\n",
    "            a_end = 1\n",
    "            if len(current_chunk) > 1:\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            \n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "            \n",
    "            # 50% 확률로 tokens_a와 tokens_b의 순서를 swap하여 NSP의 negative case를 만듭니다.\n",
    "            if random.random() < 0.5:\n",
    "                is_next = 0  # 두 문장이 이어지지 않는 경우\n",
    "                tokens_a, tokens_b = tokens_b, tokens_a\n",
    "            else:\n",
    "                is_next = 1  # 원래 이어진 경우\n",
    "            \n",
    "            # tokens_a와 tokens_b의 길이가 max_seq를 넘지 않도록 trim 합니다.\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            # assert len(tokens_a) > 0\n",
    "            # assert len(tokens_b) > 0\n",
    "            if len(tokens_a) == 0 or len(tokens_b) == 0:\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "                continue\n",
    "            \n",
    "            # special token 추가: [CLS] + tokens_a + [SEP] + tokens_b + [SEP]\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            \n",
    "            # 전체 tokens 중, [CLS]와 [SEP]를 제외한 토큰의 mask 개수를 mask_prob에 따라 결정합니다.\n",
    "            mask_cnt = int((len(tokens) - 3) * mask_prob)\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "            \n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "            \n",
    "            # 다음 chunk 생성을 위해 초기화합니다.\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [28, 29, 30, 31, 38, 39, 50, 57, 58], 'mask_label': ['▁다루', '어진', '다', '.', '▁탄', '소로', '▁원래', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [28, 29, 30, 31, 40, 53, 54, 55, 56], 'mask_label': ['▁다루', '어진', '다', '.', '▁이루어진', '▁화', '합', '물', '은']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 4, 5, 28, 29, 30, 31], 'mask_label': ['으로', '▁자리', '잡', '았다', '.', '▁다루', '어진', '다', '.']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 33, 34, 35, 36, 37, 50, 59, 60], 'mask_label': ['으로', '▁유', '기', '화', '학', '은', '▁원래', '▁동물', '로부터']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁이들은', '계는', '휘', '들에게', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 28, 29, 30, 31], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 23, 24, 25, 26, 27, 44, 45, 50], 'mask_label': ['으로', '▁유', '기', '화', '학', '에서', '▁연구', '하는', '▁원래']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁요구', 'v', '넉', '才', '始', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁북', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 16, 17, 18, 19, 20, 40, 51, 52], 'mask_label': ['으로', '▁고', '분', '자', '물', '질', '▁이루어진', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [16, 17, 18, 19, 20, 51, 52, 61, 62], 'mask_label': ['▁고', '분', '자', '물', '질', '▁유', '기', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', 'Κ', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '욤', '리학', '▁육군', '▁어느', '정', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 16, 17, 18, 19, 20, 50, 51, 52], 'mask_label': ['으로', '▁고', '분', '자', '물', '질', '▁원래', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '으로', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 3, 4, 5, 6, 7, 8, 9, 50], 'mask_label': ['▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁원래']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (3) 데이터셋 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = '/home/downtown/aiffel/BERT/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80731549a274758955e1b2d2f844056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상', '수']\n",
      "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
      "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
      "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
      "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab002ba20914ccc96fdeb1a8c6dfe36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '▁곡', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 4, 5, 40, 50, 57, 58], 'mask_label': ['으로', '▁자리', '잡', '았다', '.', '▁이루어진', '▁원래', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [46, 47, 48, 49, 50, 53, 54, 55, 56], 'mask_label': ['▁분', '과', '이다', '.', '▁원래', '▁화', '합', '물', '은']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 6, 7, 8, 9, 40, 50, 57, 58], 'mask_label': ['으로', '▁플', '라스', '틱', ',', '▁이루어진', '▁원래', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 3, 4, 5, 21, 22, 51, 52], 'mask_label': ['으로', '▁자리', '잡', '았다', '.', '▁등', '도', '▁유', '기']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 23, 24, 25, 26, 27, 40, 44, 45], 'mask_label': ['으로', '▁유', '기', '화', '학', '에서', '▁이루어진', '▁연구', '하는']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '늑', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 40, 41, 42, 43, 57, 58, 59, 60], 'mask_label': ['으로', '▁이루어진', '▁화', '합', '물을', '▁식물', '이나', '▁동물', '로부터']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 38, 39, 51, 52], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁탄', '소로', '▁유', '기']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 8, 9, 21, 22, 44, 45, 50], 'mask_label': ['▁플', '라스', '틱', ',', '▁등', '도', '▁연구', '하는', '▁원래']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [44, 45, 50, 53, 54, 55, 56, 61, 62], 'mask_label': ['▁연구', '하는', '▁원래', '▁화', '합', '물', '은', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 16, 17, 18, 19, 20, 50, 51, 52], 'mask_label': ['으로', '▁고', '분', '자', '물', '질', '▁원래', '▁유', '기']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 8, 9, 44, 45, 50, 57, 58], 'mask_label': ['▁플', '라스', '틱', ',', '▁연구', '하는', '▁원래', '▁식물', '이나']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [16, 17, 18, 19, 20, 44, 45, 51, 52], 'mask_label': ['▁고', '분', '자', '물', '질', '▁연구', '하는', '▁유', '기']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 아래 주석에 따라 코드를 완성해주세요.\n",
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if len(doc) > 0:\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    tokens = vocab.encode_as_pieces(line)\n",
    "                    doc.append(tokens)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a29ac148084e1b804a7bafc97e5273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pretrain_json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/downtown/aiffel/BERT/bert_pre_train_project.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmake_pretrain_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrain_json_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 29\u001b[0m, in \u001b[0;36mmake_pretrain_data\u001b[0;34m(vocab, in_file, out_file, n_seq, mask_prob)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# line이 빈줄 일 경우 (새로운 단락)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(doc) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 29\u001b[0m         \u001b[43msave_pretrain_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m         doc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m, in \u001b[0;36mmake_pretrain_data.<locals>.save_pretrain_instances\u001b[0;34m(out_f, doc)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_pretrain_instances\u001b[39m(out_f, doc):\n\u001b[0;32m----> 5\u001b[0m     instances \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pretrain_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m instance \u001b[38;5;129;01min\u001b[39;00m instances:\n\u001b[1;32m      7\u001b[0m         out_f\u001b[38;5;241m.\u001b[39mwrite(json\u001b[38;5;241m.\u001b[39mdumps(instance, ensure_ascii\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "Cell \u001b[0;32mIn[18], line 40\u001b[0m, in \u001b[0;36mcreate_pretrain_instances\u001b[0;34m(vocab, doc, n_seq, mask_prob, vocab_list)\u001b[0m\n\u001b[1;32m     38\u001b[0m trim_tokens(tokens_a, tokens_b, max_seq)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens_a) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens_b) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# special token 추가: [CLS] + tokens_a + [SEP] + tokens_b + [SEP]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m tokens_a \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m tokens_b \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_json_path = '/home/downtown/aiffel/BERT/bert_pre_train_project.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5036bcbb44e400a895458c74ea7e1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = '/home/downtown/aiffel/BERT/bert_pre_train_project.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918140"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5dfd16e8d540679ed93a361938c2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m label_nsp \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_next\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# mlm label\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m mask_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[1;32m     17\u001b[0m mask_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([vocab\u001b[38;5;241m.\u001b[39mpiece_to_id(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_label\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m     18\u001b[0m label_mlm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_seq, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fucklms/lib/python3.8/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    300\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.int는 Numpy 1.20 버전부터 deprecated 됨    \n",
    "np.int -> int or np.int32 or np.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d551194caf4fb5aa4875c5ea21f9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁지', '미', '▁카', '터', '[SEP]', '[MASK]', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '[MASK]', '[MASK]', '[MASK]', '▁대통령', '▁(19', '7', '7', '년', '▁~', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁지', '미', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁컨', '靖', '▁공', '과', '대학교', '를', '[MASK]', '[MASK]', '[MASK]', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '[MASK]', '[MASK]', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 31, 32, 33, 40, 41, 42, 43, 44, 65, 66, 71, 72, 73, 113, 114, 116, 117], 'mask_label': ['▁제임스', '▁3', '9', '번째', '▁1981', '년', ')', '이다', '.', '▁조지', '아', '▁졸업', '하였다', '.', '▁가', '꿔', '▁돈', '을']}\n",
      "enc_token: [5, 18, 3686, 207, 3714, 4, 6, 1042, 103, 3610, 3686, 3718, 207, 3714, 37, 3418, 416, 810, 3666, 3625, 131, 3662, 7, 3629, 203, 241, 3602, 1114, 3724, 788, 243, 6, 6, 6, 663, 1647, 3682, 3682, 3625, 203, 6, 6, 6, 6, 6, 18, 3686, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 1834, 5542, 41, 3644, 830, 3624, 6, 6, 6, 13, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 593, 6, 6, 399, 1927, 3607, 813, 17, 3599, 307, 587, 931, 103, 4313, 4290, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0 3324    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0   49 3632  796    0    0    0    0    0    0 3008 3625\n",
      " 3616   16 3599    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 1755 3630    0    0    0\n",
      "    0 1135   52 3599    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   21 5007    0 1927 3607    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1976', '년', '▁대통령', '▁선거', '에', '[MASK]', '[MASK]', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '[MASK]', '[MASK]', '▁내', '세', '워', ',', '[MASK]', '[MASK]', '▁누', '르고', '▁당선', '되었다', '.', '[SEP]', '▁196', '2', '년', '▁조지', '아', '[MASK]', '▁상', '원', '[MASK]', '[MASK]', '[MASK]', '▁낙', '선', '하나', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '▁입', '증', '하게', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '[MASK]', '[MASK]', '[MASK]', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '▁되', '기', '▁전', '▁조지', '아', '주', '▁상', '원의', '원을', '[MASK]', '[MASK]', '▁연', '임', '했으며', ',', '[MASK]', '[MASK]', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '[MASK]', '▁흑', '인', '▁등', '용', '법을', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 16, 17, 22, 23, 35, 38, 39, 40, 62, 63, 64, 93, 94, 99, 100, 121], 'mask_label': ['▁민주', '당', '▁정책', '으로', '▁포', '드를', '▁주', '▁의원', '▁선거', '에서', '▁조지', '아', '▁주', '▁두', '번', '▁1971', '년부터', '▁사는']}\n",
      "enc_token: [5, 3306, 3625, 663, 822, 3600, 6, 6, 958, 3603, 117, 3674, 54, 75, 4089, 238, 6, 6, 114, 3692, 3964, 3604, 6, 6, 807, 2056, 2387, 43, 3599, 4, 386, 3619, 3625, 1755, 3630, 6, 76, 3667, 6, 6, 6, 1567, 3668, 3294, 13, 822, 3608, 2386, 2163, 3596, 3671, 969, 213, 3929, 173, 607, 2387, 317, 3604, 386, 3673, 3625, 6, 6, 6, 18, 3620, 822, 3600, 1567, 3668, 1447, 1921, 3625, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 663, 3597, 450, 3614, 25, 1755, 3630, 3646, 76, 955, 928, 6, 6, 61, 3773, 530, 3604, 6, 6, 3409, 673, 1755, 3630, 18, 982, 2711, 31, 3599, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 243, 3600, 6, 1733, 3628, 50, 3717, 2046, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0 1114 3724    0    0    0    0    0    0\n",
      "    0    0 1421    9    0    0    0    0  119 1486    0    0    0    0\n",
      "    0    0    0    0    0    0    0   37    0    0 2378  822   10    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0 1755 3630   37    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  157 3821    0    0    0\n",
      "    0 3372  523    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0 3554    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', ',', '▁캠', '프', '▁데이', '비', '드에서', '▁안', '와', '르', '▁사', '다', '트', '▁대통령', '과', '[MASK]', '[MASK]', '[MASK]', '▁베', '긴', '▁수상', '과', '▁함께', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '▁그러나', '▁이것은', '▁공', '화', '당', '과', '▁미국의', '▁떠나', 'Г', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '군에', '▁소련', '과', '▁제', '2', '차', '▁전략', '▁무', '기', '[MASK]', '[MASK]', '[MASK]', '▁조', '인', '했다', '.', '[SEP]', '▁카', '터', '는', '▁1970', '년대', '▁후반', '▁당시', '▁대한민국', '▁등', '▁인', '권', '▁후', '진', '국의', '▁국민', '들의', '▁인', '권을', '[MASK]', '[MASK]', '[MASK]', '▁위해', '▁노력', '했으며', ',', '▁취임', '▁이후', '▁계속', '해서', '▁도', '덕', '정', '치를', '▁내', '세', '웠다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 16, 17, 46, 47, 57, 58, 59, 60, 61, 62, 73, 82, 83, 84, 108, 109, 110], 'mask_label': ['▁메', '나', '헴', '▁유대', '인', '▁1979', '년', '▁백', '악', '관', '에서', '▁또한', '▁제한', '▁협', '상에', '▁지', '키', '기']}\n",
      "enc_token: [5, 3604, 2432, 3721, 965, 3694, 3552, 172, 3665, 3699, 15, 3598, 3677, 663, 3644, 6, 6, 6, 271, 4099, 1011, 3644, 280, 35, 3658, 232, 934, 521, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 330, 1487, 41, 3683, 3724, 3644, 679, 2613, 6962, 164, 1314, 141, 3720, 3607, 1213, 4174, 3598, 3599, 6, 6, 6, 6, 6, 6, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 1501, 1302, 3644, 30, 3619, 3751, 2835, 107, 3614, 6, 6, 6, 53, 3628, 31, 3599, 4, 207, 3714, 3602, 1921, 596, 1840, 316, 410, 50, 42, 3830, 81, 3713, 137, 968, 247, 42, 917, 6, 6, 6, 231, 3375, 530, 3604, 2659, 165, 785, 874, 75, 4089, 3642, 1233, 114, 3692, 1853, 3599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  334 3637 5887    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0 2670 3628    0    0    0    0    0    0    0    0\n",
      "    0 2995 3625  456 3928 3708   10    0    0    0    0    0    0    0\n",
      "    0    0    0  276    0    0    0    0    0    0    0    0 1956  617\n",
      " 1824    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0   18 3793 3614    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁구', '출', '▁실패', '를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거', '에서', '▁공', '화', '당의', '▁로', '널', '드', '▁레이', '건', '▁후보', '에게', '[MASK]', '[MASK]', '▁결국', '[MASK]', '[MASK]', '▁실패', '했다', '.', '▁또한', '▁임', '기', '[MASK]', '[MASK]', '▁터', '진', '▁소련', '의', '▁아', '프가', '니', '스탄', '▁침공', '▁사건', '으로', '[MASK]', '[MASK]', '[MASK]', '▁하계', '▁올림픽', '에', '▁반', '공', '국', '가', '들의', '▁보이', '콧', '을', '▁내', '세', '웠다', '.', '[SEP]', '▁지', '미', '▁카', '터', '는', '▁대한민국', '과의', '▁관계', '에서도', '▁중요한', '▁영향을', '▁미', '쳤', '던', '▁대통령', '▁중', '▁하나', '다', '.', '▁인', '권', '[MASK]', '[MASK]', '▁주', '한', '미', '군', '▁철', '수', '▁문제', '로', '[MASK]', '▁한', '미', '▁관계', '가', '[MASK]', '[MASK]', '[MASK]', '▁했다', '.', '▁1978', '년', '▁대한민국', '에', '▁대한', '▁북한', '의', '▁위협', '에', '▁대', '비', '해', '▁한', '미', '연합', '사를', '▁창설', '하면서', ',', '▁1982', '년까지', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 22, 23, 25, 26, 33, 34, 46, 47, 48, 86, 87, 92, 93, 96, 101, 102, 103], 'mask_label': ['질', '▁', '져', '▁재', '선에', '▁말', '기에', '▁인해', '▁1980', '년', '▁문제', '와', '▁철', '수', '▁한때', '▁불', '편', '하기도']}\n",
      "enc_token: [5, 6, 73, 3771, 1579, 3624, 1827, 1640, 3625, 663, 822, 10, 41, 3683, 1547, 194, 4044, 3681, 1169, 3803, 958, 113, 6, 6, 875, 6, 6, 1579, 31, 3599, 276, 273, 3614, 6, 6, 870, 3713, 1302, 3601, 26, 2986, 3733, 1323, 3232, 636, 9, 6, 6, 6, 2219, 779, 3600, 141, 3670, 3643, 3608, 247, 3052, 4805, 3607, 114, 3692, 1853, 3599, 4, 18, 3686, 207, 3714, 3602, 410, 786, 704, 643, 1165, 1063, 55, 4219, 3781, 663, 35, 324, 3598, 3599, 42, 3830, 6, 6, 37, 3612, 3686, 3722, 380, 3636, 550, 3603, 6, 34, 3686, 704, 3608, 6, 6, 6, 345, 3599, 3331, 3625, 410, 3600, 92, 1876, 3601, 3038, 3600, 14, 3694, 3645, 34, 3686, 2569, 451, 3574, 421, 3604, 2760, 673, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0 3892    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 3596 3944    0  174 2087    0\n",
      "    0    0    0    0    0  150  329    0    0    0    0    0    0    0\n",
      "    0    0    0    0  751 1640 3625    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  550 3665    0    0    0    0  380 3636    0    0 3590    0\n",
      "    0    0    0  128 3877  863    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁밴', '스', '▁국', '무', '장', '관을', '▁조', '문', '사', '절', '로', '▁파견', '했다', '.', '▁12', '·', '12', '▁군사', '▁반란', '과', '▁5', '.', '17', '▁쿠', '데', '타', '에', '콤', '▁초기', '에는', '▁강', '하게', '▁비난', '했으나', ',', '▁미국', '▁정부', '가', '▁신', '군', '부를', '▁설', '득', '하는데', ',', '▁한', '계가', '▁있었고', '▁결국', '▁', '묵', '인', '하는', '▁', '듯', '한', '[MASK]', '[MASK]', '▁보이', '게', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁퇴', '임', '▁이후', '[MASK]', '▁자', '원을', '악', '[MASK]', '[MASK]', '▁비', '영', '리', '▁기', '구', '인', '▁카', '터', '▁재', '단을', '▁설립', '한', '▁뒤', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁위해', '▁제', '▁3', '세', '계의', '▁선거', '▁감', '시', '▁활동', '[MASK]', '▁기', '니', '▁벌', '레', '에', '▁의한', '▁드', '라', '쿤', '쿠', '르', '스', '▁질', '병', '▁방', '재', '를', '▁위해', '▁힘', '썼', '다', '.', '▁미국의', '▁빈', '곤', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [28, 36, 57, 58, 61, 62, 63, 68, 71, 72, 73, 87, 88, 89, 90, 91, 101, 107], 'mask_label': ['▁대해', '▁미국', '▁태', '도를', '▁', '됐다', '.', '▁민간', '▁적극', '▁활용', '한', '▁민주', '주의', '▁실', '현', '을', '▁및', '▁의한']}\n",
      "enc_token: [5, 1228, 3626, 79, 3725, 3651, 1657, 53, 3697, 3620, 3931, 3603, 2338, 31, 3599, 196, 3873, 1335, 1250, 2342, 3644, 94, 3599, 1695, 927, 3736, 3732, 3600, 4450, 1348, 66, 139, 173, 3560, 1003, 3604, 243, 513, 3608, 90, 3722, 1191, 178, 4059, 1294, 3604, 34, 2681, 2492, 875, 3596, 4502, 3628, 38, 3596, 4360, 3612, 6, 6, 3052, 3669, 6, 6, 6, 4, 1382, 3773, 165, 6, 40, 928, 3928, 6, 6, 77, 3715, 3622, 24, 3653, 3628, 207, 3714, 174, 1574, 686, 3612, 339, 6, 6, 6, 6, 6, 231, 30, 49, 3692, 1654, 822, 209, 3623, 375, 6, 24, 3733, 813, 3740, 3600, 1332, 311, 3635, 4956, 3937, 3699, 3626, 761, 3886, 95, 3729, 3624, 231, 947, 4437, 3598, 3599, 679, 1412, 4234, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  433    0    0    0    0    0    0    0  243    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0  227  701    0    0 3596 1027 3599    0    0    0    0 3174    0\n",
      "    0 2929 2523 3612    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 1114  238  158 3756 3607    0    0    0    0    0    0\n",
      "    0    0    0  228    0    0    0    0    0 1332    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '상을', '▁거부', '하면서', '▁사', '태', '의', '▁위', '기를', '▁초', '래', '한', '[MASK]', '▁및', '▁단', '체를', '▁직접', '[MASK]', '▁분', '쟁', '의', '[MASK]', '[MASK]', '▁근', '본', '적으로', '▁해결', '하기', '▁위해', '▁힘', '썼', '다', '.', '▁이', '▁과정에서', '▁미국', '▁행정', '부와', '▁갈', '등', '을', '▁보', '이기도', '▁했지만', ',', '▁전', '직', '▁대통령', '의', '▁권', '한', '과', '▁재', '야', '▁유명', '▁인사', '들의', '▁활약', '으로', '▁해결', '해', '▁나', '갔다', '.', '[SEP]', '▁카', '터', '는', '▁카', '터', '▁행정', '부', '▁이후', '▁미국', '이', '▁북', '핵', '▁위', '기', ',', '▁코', '소', '보', '▁전쟁', ',', '[MASK]', '[MASK]', '▁전쟁', '과', '▁같이', '▁미국', '이', '▁군사', '적', '▁행', '동을', '▁최', '후', '로', '▁선택', '하는', '[MASK]', '[MASK]', '▁사고', '를', '▁버', '리고', '▁군사', '적', '▁행', '동을', '▁나무', 'ğ', '廣', '▁행', '위에', '▁대해', '[MASK]', '[MASK]', '▁유', '감을', '▁표시', '▁하며', '[MASK]', '▁군사', '적', '▁활동', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 17, 21, 22, 59, 60, 85, 86, 101, 102, 111, 112, 113, 117, 118, 123, 124, 125], 'mask_label': ['▁인물', '▁만나', '▁원', '인을', '▁해결', '해', '▁이', '라크', '▁전통', '적', '▁선', '행', '하는', '▁깊', '은', '▁미국의', '▁군사', '적']}\n",
      "enc_token: [5, 460, 2324, 421, 15, 3800, 3601, 45, 333, 192, 3808, 3612, 6, 228, 164, 1396, 1069, 6, 147, 3972, 3601, 6, 6, 387, 3759, 127, 2317, 167, 231, 947, 4437, 3598, 3599, 8, 2208, 243, 895, 2576, 742, 3709, 3607, 47, 1304, 3379, 3604, 25, 3802, 663, 3601, 476, 3612, 3644, 174, 3775, 939, 3329, 247, 1102, 9, 2317, 3645, 58, 1133, 3599, 4, 207, 3714, 3602, 207, 3714, 895, 3638, 165, 243, 3597, 251, 4166, 45, 3614, 3604, 258, 3688, 3672, 506, 3604, 6, 6, 506, 3644, 733, 243, 3597, 1250, 3657, 236, 1629, 130, 3706, 3603, 1715, 38, 6, 6, 1646, 3624, 407, 999, 1250, 3657, 236, 1629, 2391, 7564, 4900, 236, 1157, 433, 6, 6, 46, 2196, 2466, 1368, 6, 1250, 3657, 375, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0 1178    0\n",
      "    0    0    0 2142    0    0    0  129 1171    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 2317 3645    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    8 3553    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0 1306 3657    0    0    0    0    0    0    0    0   57\n",
      " 3752   38    0    0    0 1910 3613    0    0    0    0  679 1250 3657\n",
      "    0    0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int32)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int32)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int32, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int32)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int32)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int32, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e47b3efb75f4e769382b5b48d34949a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   18, 3686,  207, 3714,    4,    6, 1042,  103, 3610, 3686,\n",
       "         3718,  207, 3714,   37, 3418,  416,  810, 3666, 3625,  131, 3662,\n",
       "            7, 3629,  203,  241, 3602, 1114, 3724,  788,  243,    6,    6,\n",
       "            6,  663, 1647, 3682, 3682, 3625,  203,    6,    6,    6,    6,\n",
       "            6,   18, 3686,  207, 3714, 3602, 1755, 3630, 3646,  630, 3714,\n",
       "         3565, 3835,  429, 3740, 3628, 3626, 1369,   10, 1605, 3599, 1834,\n",
       "         5542,   41, 3644,  830, 3624,    6,    6,    6,   13,   81,   87,\n",
       "         1501, 2247,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,  168, 3877,  414,  165, 1697, 4290, 3873,\n",
       "         3703, 3683,  593,    6,    6,  399, 1927, 3607,  813,   17, 3599,\n",
       "          307,  587,  931,  103, 4313, 4290,    4], dtype=int32),\n",
       " memmap([   5,   13,   81, 3604,   15, 3784,    6,    6,    6,   13,  316,\n",
       "         1425,  173,  305, 3620, 1395,  149, 3607,   19,  805, 3596, 4904,\n",
       "         3750, 3603, 4065,  115, 3617, 3756, 3596, 4639, 1364, 3627,  991,\n",
       "         3616, 3600,    7, 3614, 3746,    9,    6,    6, 1345, 3604,  848,\n",
       "         3784, 3833,    6,    6,    6,   12, 3614, 3746,  836, 3596, 4904,\n",
       "         3750, 3603, 4065,  115, 3600, 2972,  173,  351, 3599,    4,  848,\n",
       "         3784, 3833,    8, 3637, 3676,  848, 3784, 1931,   58, 3676,  416,\n",
       "         2316, 3619, 3625, 3617, 3744, 4335,   12, 3625, 3616,  175, 3662,\n",
       "            7, 3629,  203,  578, 3652, 3625, 3617, 4148, 3665,  143, 3625,\n",
       "         3616,    6,    6,  342, 3629, 3616, 3602,  176,    6,    6,    6,\n",
       "            6,    6,    6, 3451, 1633,    6,    6, 1644, 3608,  547, 3423,\n",
       "          765,  815, 3604,  752, 3608, 3604,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([   0,    0,    0,    0,    0,    0, 3324,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,   49, 3632,\n",
       "          796,    0,    0,    0,    0,    0,    0, 3008, 3625, 3616,   16,\n",
       "         3599,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1755,\n",
       "         3630,    0,    0,    0,    0, 1135,   52, 3599,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,   21, 5007,    0, 1927, 3607,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,   68, 3238, 3602,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, 2972,  173,    0,    0,    0,\n",
       "            0,    0,    8, 3637, 2263,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,  131, 3662,    0,    0,    0,    0,    0,  334,  829, 1115,\n",
       "         3665, 1381, 4148,    0,    0,  375,  671,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 주석과 코드를 참조하여 아래 클래스를 완성해주세요.\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain 용 BERT 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아주 작은 pretrain용 BERT 모델(test_model)을 생성하여 동작을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 3s 35ms/step - loss: 9.7652 - nsp_loss: 0.7289 - mlm_loss: 9.0363 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 8.5876 - nsp_loss: 0.6329 - mlm_loss: 7.9547 - nsp_acc: 0.8000 - mlm_acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd2357ca220>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=800000, warmup_steps=50000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=800000, warmup_steps=50000)\n",
    "lrs = []\n",
    "for step_num in range(800000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " enc_tokens (InputLayer)     [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " segments (InputLayer)       [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " bert (BERT)                 ((None, 256),                4485632   ['enc_tokens[0][0]',          \n",
      "                              (None, None, 8007))                    'segments[0][0]']            \n",
      "                                                                                                  \n",
      " pooled_nsp (PooledOutput)   (None, 2)                    66304     ['bert[0][0]']                \n",
      "                                                                                                  \n",
      " nsp (Softmax)               (None, 2)                    0         ['pooled_nsp[0][0]']          \n",
      "                                                                                                  \n",
      " mlm (Softmax)               (None, None, 8007)           0         ['bert[0][1]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4551936 (17.36 MB)\n",
      "Trainable params: 4551936 (17.36 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 800000\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2486 - nsp_loss: 0.7019 - mlm_loss: 181.5468 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 1: mlm_lm_acc improved from -inf to 0.00000, saving model to /home/downtown/aiffel/BERT/models/bert_pre_train.hdf5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 182.2486 - nsp_loss: 0.7019 - mlm_loss: 181.5468 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5538 - nsp_loss: 0.6978 - mlm_loss: 181.8560 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 2: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 182.5538 - nsp_loss: 0.6978 - mlm_loss: 181.8560 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1701 - nsp_loss: 0.6994 - mlm_loss: 181.4707 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 3: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 182.1701 - nsp_loss: 0.6994 - mlm_loss: 181.4707 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5108 - nsp_loss: 0.6968 - mlm_loss: 181.8140 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 4: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 182.5108 - nsp_loss: 0.6968 - mlm_loss: 181.8140 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1849 - nsp_loss: 0.6989 - mlm_loss: 181.4860 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 5: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 182.1849 - nsp_loss: 0.6989 - mlm_loss: 181.4860 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3080 - nsp_loss: 0.6964 - mlm_loss: 181.6117 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 6: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 182.3080 - nsp_loss: 0.6964 - mlm_loss: 181.6117 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0118 - nsp_loss: 0.6981 - mlm_loss: 181.3137 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 7: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.0118 - nsp_loss: 0.6981 - mlm_loss: 181.3137 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3519 - nsp_loss: 0.6963 - mlm_loss: 181.6556 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 8: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 182.3519 - nsp_loss: 0.6963 - mlm_loss: 181.6556 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0384 - nsp_loss: 0.6967 - mlm_loss: 181.3417 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 9: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 182.0384 - nsp_loss: 0.6967 - mlm_loss: 181.3417 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5254 - nsp_loss: 0.6934 - mlm_loss: 181.8319 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 10: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 182.5254 - nsp_loss: 0.6934 - mlm_loss: 181.8319 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9484 - nsp_loss: 0.6988 - mlm_loss: 181.2496 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 11: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - -3s -2587270us/step - loss: 181.9484 - nsp_loss: 0.6988 - mlm_loss: 181.2496 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5999 - nsp_loss: 0.7009 - mlm_loss: 181.8990 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 12: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.5999 - nsp_loss: 0.7009 - mlm_loss: 181.8990 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4285 - nsp_loss: 0.6986 - mlm_loss: 181.7298 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 13: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.4285 - nsp_loss: 0.6986 - mlm_loss: 181.7298 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2295 - nsp_loss: 0.6941 - mlm_loss: 181.5354 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 14: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.2295 - nsp_loss: 0.6941 - mlm_loss: 181.5354 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.6428 - nsp_loss: 0.6992 - mlm_loss: 181.9437 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 15: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.6428 - nsp_loss: 0.6992 - mlm_loss: 181.9437 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2326 - nsp_loss: 0.7032 - mlm_loss: 181.5294 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 16: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.2326 - nsp_loss: 0.7032 - mlm_loss: 181.5294 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5162 - nsp_loss: 0.7013 - mlm_loss: 181.8150 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 17: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.5162 - nsp_loss: 0.7013 - mlm_loss: 181.8150 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1132 - nsp_loss: 0.7024 - mlm_loss: 181.4108 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 18: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.1132 - nsp_loss: 0.7024 - mlm_loss: 181.4108 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4704 - nsp_loss: 0.6948 - mlm_loss: 181.7755 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 19: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.4704 - nsp_loss: 0.6948 - mlm_loss: 181.7755 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1626 - nsp_loss: 0.7001 - mlm_loss: 181.4625 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 20: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.1626 - nsp_loss: 0.7001 - mlm_loss: 181.4625 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4397 - nsp_loss: 0.6962 - mlm_loss: 181.7436 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 21: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.4397 - nsp_loss: 0.6962 - mlm_loss: 181.7436 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4801 - nsp_loss: 0.7002 - mlm_loss: 181.7799 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 22: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.4801 - nsp_loss: 0.7002 - mlm_loss: 181.7799 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2542 - nsp_loss: 0.6954 - mlm_loss: 181.5588 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 23: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2542 - nsp_loss: 0.6954 - mlm_loss: 181.5588 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2895 - nsp_loss: 0.6987 - mlm_loss: 181.5908 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 24: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2895 - nsp_loss: 0.6987 - mlm_loss: 181.5908 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3770 - nsp_loss: 0.6964 - mlm_loss: 181.6806 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 25: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.3770 - nsp_loss: 0.6964 - mlm_loss: 181.6806 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0710 - nsp_loss: 0.6981 - mlm_loss: 181.3729 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 26: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.0710 - nsp_loss: 0.6981 - mlm_loss: 181.3729 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0855 - nsp_loss: 0.6995 - mlm_loss: 181.3859 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 27: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.0855 - nsp_loss: 0.6995 - mlm_loss: 181.3859 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3648 - nsp_loss: 0.6967 - mlm_loss: 181.6681 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 28: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.3648 - nsp_loss: 0.6967 - mlm_loss: 181.6681 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2036 - nsp_loss: 0.6999 - mlm_loss: 181.5037 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 29: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2036 - nsp_loss: 0.6999 - mlm_loss: 181.5037 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3813 - nsp_loss: 0.6998 - mlm_loss: 181.6815 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 30: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.3813 - nsp_loss: 0.6998 - mlm_loss: 181.6815 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0797 - nsp_loss: 0.6977 - mlm_loss: 181.3820 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 31: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.0797 - nsp_loss: 0.6977 - mlm_loss: 181.3820 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3400 - nsp_loss: 0.6988 - mlm_loss: 181.6412 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 32: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.3400 - nsp_loss: 0.6988 - mlm_loss: 181.6412 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.6504 - nsp_loss: 0.6974 - mlm_loss: 181.9530 - nsp_acc: 0.6000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 33: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.6504 - nsp_loss: 0.6974 - mlm_loss: 181.9530 - nsp_acc: 0.6000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3269 - nsp_loss: 0.6966 - mlm_loss: 181.6303 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 34: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.3269 - nsp_loss: 0.6966 - mlm_loss: 181.6303 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4100 - nsp_loss: 0.6921 - mlm_loss: 181.7179 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 35: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.4100 - nsp_loss: 0.6921 - mlm_loss: 181.7179 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5936 - nsp_loss: 0.6960 - mlm_loss: 181.8976 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 36: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.5936 - nsp_loss: 0.6960 - mlm_loss: 181.8976 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3133 - nsp_loss: 0.6986 - mlm_loss: 181.6147 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 37: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.3133 - nsp_loss: 0.6986 - mlm_loss: 181.6147 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1303 - nsp_loss: 0.6955 - mlm_loss: 181.4348 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 38: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1303 - nsp_loss: 0.6955 - mlm_loss: 181.4348 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9622 - nsp_loss: 0.6921 - mlm_loss: 181.2701 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 39: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 181.9622 - nsp_loss: 0.6921 - mlm_loss: 181.2701 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2204 - nsp_loss: 0.6980 - mlm_loss: 181.5223 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 40: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2204 - nsp_loss: 0.6980 - mlm_loss: 181.5223 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1146 - nsp_loss: 0.6969 - mlm_loss: 181.4177 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 41: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1146 - nsp_loss: 0.6969 - mlm_loss: 181.4177 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2859 - nsp_loss: 0.6965 - mlm_loss: 181.5894 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 42: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.2859 - nsp_loss: 0.6965 - mlm_loss: 181.5894 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1821 - nsp_loss: 0.6947 - mlm_loss: 181.4874 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 43: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1821 - nsp_loss: 0.6947 - mlm_loss: 181.4874 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1707 - nsp_loss: 0.7016 - mlm_loss: 181.4690 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 44: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1707 - nsp_loss: 0.7016 - mlm_loss: 181.4690 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1925 - nsp_loss: 0.6987 - mlm_loss: 181.4938 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 45: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1925 - nsp_loss: 0.6987 - mlm_loss: 181.4938 - nsp_acc: 0.2000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2671 - nsp_loss: 0.6976 - mlm_loss: 181.5695 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 46: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.2671 - nsp_loss: 0.6976 - mlm_loss: 181.5695 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.8269 - nsp_loss: 0.7005 - mlm_loss: 181.1264 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 47: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 181.8269 - nsp_loss: 0.7005 - mlm_loss: 181.1264 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9271 - nsp_loss: 0.7016 - mlm_loss: 181.2255 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 48: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 181.9271 - nsp_loss: 0.7016 - mlm_loss: 181.2255 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0962 - nsp_loss: 0.6965 - mlm_loss: 181.3996 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 49: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.0962 - nsp_loss: 0.6965 - mlm_loss: 181.3996 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1276 - nsp_loss: 0.7012 - mlm_loss: 181.4263 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 50: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1276 - nsp_loss: 0.7012 - mlm_loss: 181.4263 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5139 - nsp_loss: 0.6984 - mlm_loss: 181.8155 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 51: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.5139 - nsp_loss: 0.6984 - mlm_loss: 181.8155 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0994 - nsp_loss: 0.7004 - mlm_loss: 181.3989 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 52: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 182.0994 - nsp_loss: 0.7004 - mlm_loss: 181.3989 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1763 - nsp_loss: 0.6958 - mlm_loss: 181.4805 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 53: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1763 - nsp_loss: 0.6958 - mlm_loss: 181.4805 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0501 - nsp_loss: 0.6988 - mlm_loss: 181.3513 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 54: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.0501 - nsp_loss: 0.6988 - mlm_loss: 181.3513 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2673 - nsp_loss: 0.6967 - mlm_loss: 181.5706 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 55: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2673 - nsp_loss: 0.6967 - mlm_loss: 181.5706 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5899 - nsp_loss: 0.7009 - mlm_loss: 181.8890 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 56: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.5899 - nsp_loss: 0.7009 - mlm_loss: 181.8890 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2741 - nsp_loss: 0.6987 - mlm_loss: 181.5754 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 57: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2741 - nsp_loss: 0.6987 - mlm_loss: 181.5754 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2089 - nsp_loss: 0.6935 - mlm_loss: 181.5154 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 58: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2089 - nsp_loss: 0.6935 - mlm_loss: 181.5154 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5501 - nsp_loss: 0.6969 - mlm_loss: 181.8532 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 59: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.5501 - nsp_loss: 0.6969 - mlm_loss: 181.8532 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2566 - nsp_loss: 0.6930 - mlm_loss: 181.5636 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 60: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.2566 - nsp_loss: 0.6930 - mlm_loss: 181.5636 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4109 - nsp_loss: 0.6971 - mlm_loss: 181.7138 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 61: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.4109 - nsp_loss: 0.6971 - mlm_loss: 181.7138 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1395 - nsp_loss: 0.6935 - mlm_loss: 181.4460 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 62: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1395 - nsp_loss: 0.6935 - mlm_loss: 181.4460 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3095 - nsp_loss: 0.6981 - mlm_loss: 181.6114 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 63: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.3095 - nsp_loss: 0.6981 - mlm_loss: 181.6114 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3311 - nsp_loss: 0.6993 - mlm_loss: 181.6318 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 64: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.3311 - nsp_loss: 0.6993 - mlm_loss: 181.6318 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4179 - nsp_loss: 0.6939 - mlm_loss: 181.7240 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 65: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.4179 - nsp_loss: 0.6939 - mlm_loss: 181.7240 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1329 - nsp_loss: 0.6990 - mlm_loss: 181.4339 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 66: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1329 - nsp_loss: 0.6990 - mlm_loss: 181.4339 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2280 - nsp_loss: 0.6941 - mlm_loss: 181.5339 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 67: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2280 - nsp_loss: 0.6941 - mlm_loss: 181.5339 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9790 - nsp_loss: 0.6970 - mlm_loss: 181.2820 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 68: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 181.9790 - nsp_loss: 0.6970 - mlm_loss: 181.2820 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0840 - nsp_loss: 0.6980 - mlm_loss: 181.3861 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 69: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.0840 - nsp_loss: 0.6980 - mlm_loss: 181.3861 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1466 - nsp_loss: 0.6957 - mlm_loss: 181.4509 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 70: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.1466 - nsp_loss: 0.6957 - mlm_loss: 181.4509 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1460 - nsp_loss: 0.6991 - mlm_loss: 181.4469 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 71: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.1460 - nsp_loss: 0.6991 - mlm_loss: 181.4469 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0130 - nsp_loss: 0.7036 - mlm_loss: 181.3093 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 72: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.0130 - nsp_loss: 0.7036 - mlm_loss: 181.3093 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.7314 - nsp_loss: 0.6963 - mlm_loss: 181.0352 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 73: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 181.7314 - nsp_loss: 0.6963 - mlm_loss: 181.0352 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5617 - nsp_loss: 0.6974 - mlm_loss: 181.8643 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 74: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.5617 - nsp_loss: 0.6974 - mlm_loss: 181.8643 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2015 - nsp_loss: 0.7018 - mlm_loss: 181.4997 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 75: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2015 - nsp_loss: 0.7018 - mlm_loss: 181.4997 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.8871 - nsp_loss: 0.6952 - mlm_loss: 181.1919 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 76: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 181.8871 - nsp_loss: 0.6952 - mlm_loss: 181.1919 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2078 - nsp_loss: 0.6968 - mlm_loss: 181.5110 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 77: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 182.2078 - nsp_loss: 0.6968 - mlm_loss: 181.5110 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.4334 - nsp_loss: 0.6971 - mlm_loss: 181.7363 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 78: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.4334 - nsp_loss: 0.6971 - mlm_loss: 181.7363 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9454 - nsp_loss: 0.7017 - mlm_loss: 181.2438 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 79: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 181.9454 - nsp_loss: 0.7017 - mlm_loss: 181.2438 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.7810 - nsp_loss: 0.6939 - mlm_loss: 181.0871 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 80: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 181.7810 - nsp_loss: 0.6939 - mlm_loss: 181.0871 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9815 - nsp_loss: 0.6933 - mlm_loss: 181.2882 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 81: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 181.9815 - nsp_loss: 0.6933 - mlm_loss: 181.2882 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1514 - nsp_loss: 0.6927 - mlm_loss: 181.4588 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 82: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.1514 - nsp_loss: 0.6927 - mlm_loss: 181.4588 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.1509 - nsp_loss: 0.7024 - mlm_loss: 181.4485 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 83: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.1509 - nsp_loss: 0.7024 - mlm_loss: 181.4485 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9932 - nsp_loss: 0.6965 - mlm_loss: 181.2967 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 84: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 181.9932 - nsp_loss: 0.6965 - mlm_loss: 181.2967 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0917 - nsp_loss: 0.6944 - mlm_loss: 181.3973 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 85: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.0917 - nsp_loss: 0.6944 - mlm_loss: 181.3973 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.7681 - nsp_loss: 0.6988 - mlm_loss: 181.0693 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 86: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 181.7681 - nsp_loss: 0.6988 - mlm_loss: 181.0693 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3077 - nsp_loss: 0.6929 - mlm_loss: 181.6148 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 87: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.3077 - nsp_loss: 0.6929 - mlm_loss: 181.6148 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2467 - nsp_loss: 0.6972 - mlm_loss: 181.5496 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 88: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.2467 - nsp_loss: 0.6972 - mlm_loss: 181.5496 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2803 - nsp_loss: 0.6959 - mlm_loss: 181.5844 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 89: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 182.2803 - nsp_loss: 0.6959 - mlm_loss: 181.5844 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0816 - nsp_loss: 0.6959 - mlm_loss: 181.3857 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 90: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 182.0816 - nsp_loss: 0.6959 - mlm_loss: 181.3857 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9504 - nsp_loss: 0.6969 - mlm_loss: 181.2535 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 91: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 181.9504 - nsp_loss: 0.6969 - mlm_loss: 181.2535 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.5762 - nsp_loss: 0.6941 - mlm_loss: 181.8821 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 92: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 182.5762 - nsp_loss: 0.6941 - mlm_loss: 181.8821 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.8996 - nsp_loss: 0.6967 - mlm_loss: 181.2029 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 93: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 181.8996 - nsp_loss: 0.6967 - mlm_loss: 181.2029 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9632 - nsp_loss: 0.6960 - mlm_loss: 181.2672 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 94: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 181.9632 - nsp_loss: 0.6960 - mlm_loss: 181.2672 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9356 - nsp_loss: 0.7036 - mlm_loss: 181.2320 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 95: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 181.9356 - nsp_loss: 0.7036 - mlm_loss: 181.2320 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3013 - nsp_loss: 0.7011 - mlm_loss: 181.6003 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 96: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.3013 - nsp_loss: 0.7011 - mlm_loss: 181.6003 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.3705 - nsp_loss: 0.6975 - mlm_loss: 181.6729 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 97: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.3705 - nsp_loss: 0.6975 - mlm_loss: 181.6729 - nsp_acc: 0.3000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.2869 - nsp_loss: 0.6984 - mlm_loss: 181.5885 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 98: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.2869 - nsp_loss: 0.6984 - mlm_loss: 181.5885 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 182.0042 - nsp_loss: 0.6966 - mlm_loss: 181.3076 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 99: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 182.0042 - nsp_loss: 0.6966 - mlm_loss: 181.3076 - nsp_acc: 0.5000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 181.9524 - nsp_loss: 0.6945 - mlm_loss: 181.2579 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n",
      "Epoch 100: mlm_lm_acc did not improve from 0.00000\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 181.9524 - nsp_loss: 0.6945 - mlm_loss: 181.2579 - nsp_acc: 0.4000 - mlm_lm_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Q. 모델을 학습시키고, 내용을 history에 담아주세요.\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    (enc_tokens, segments), \n",
    "    (labels_nsp, labels_mlm),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[save_weights]\n",
    ")\n",
    "# 모델 인자에는 inputs, labels, epochs, batch size, callback 이 필요해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고  \n",
    "vocab 문제 가능성   \n",
    "다른 사람들도 결과가 이렇게 나오는지, 어떻게 해결했는지 궁금하다  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fucklms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
